@article{a.SystematicReviewExplainable2023,
  title = {A Systematic Review of {{Explainable Artificial Intelligence}} Models and Applications: {{Recent}} Developments and Future Trends},
  shorttitle = {A Systematic Review of {{Explainable Artificial Intelligence}} Models and Applications},
  author = {A., Saranya and R., Subhashini},
  date = {2023-06-01},
  journaltitle = {Decision Analytics Journal},
  shortjournal = {Decision Analytics Journal},
  volume = {7},
  pages = {100230},
  issn = {2772-6622},
  doi = {10.1016/j.dajour.2023.100230},
  url = {https://www.sciencedirect.com/science/article/pii/S277266222300070X},
  urldate = {2023-11-27},
  abstract = {Artificial Intelligence (AI) uses systems and machines to simulate human intelligence and solve common real-world problems. Machine learning and deep learning are Artificial intelligence technologies that use algorithms to predict outcomes more accurately without relying on human intervention. However, the opaque black box model and cumulative model complexity can be used to achieve. Explainable Artificial Intelligence (XAI) is a term that refers to Artificial Intelligence (AI) that can provide explanations for their decision or predictions to human users. XAI aims to increase the transparency, trustworthiness and accountability of AI system, especially when they are used for high-stakes application such as healthcare, finance or security. This paper offers systematic literature review of XAI approaches with different application and observes 91 recently published articles describing XAI development and applications in healthcare, manufacturing, transportation, and finance. We investigated the Scopus, Web of Science, IEEE Xplore and PubMed databases, to find the pertinent publications published between January 2018 to October 2022. It contains the published research on XAI~modelling that were retrieved from scholarly databases using pertinent keyword searches. We think that our systematic review extends to the literature on XAI by working as a roadmap for further research in the field.},
  keywords = {Artificial Intelligence,Deep learning,Explainable Artificial Intelligence,Explanation,HealthCare,Machine learning},
  file = {/Users/ash/Zotero/storage/CQD2HE28/A. and R. - 2023 - A systematic review of Explainable Artificial Inte.pdf;/Users/ash/Zotero/storage/QVQ3T3YP/S277266222300070X.html}
}


@book{osumare_africanist_2007,
	address = {New York},
	edition = {1st ed},
	title = {The {Africanist} aesthetic in global hip-hop: power moves},
	isbn = {978-1-4039-7630-7},
	shorttitle = {The {Africanist} aesthetic in global hip-hop},
	publisher = {Palgrave Macmillan},
	author = {Osumare, Halifu},
	year = {2007},
	note = {OCLC: 70708109},
	keywords = {Check out, Hip-hop, Influence, Intercultural communication, Political aspects, Rap (Music), Social aspects},
}

@article{fiebrink_wekinator_2010,
	title = {The {Wekinator}: {A} {System} for {Real}-time, {Interactive} {Machine} {Learning} in {Music}},
	shorttitle = {The {Wekinator}},
	abstract = {We propose a demonstration of The Wekinator, our soft-ware system that enables the application of machine-learning based music information retrieval techniques to real-time musical performance, and which emphasizes a richer human-computer interaction in the design of ma-chine learning systems.},
	journal = {Proceedings of The Eleventh International Society for Music Information Retrieval Conference (ISMIR 2010)},
	author = {Fiebrink, Rebecca and Cook, Perry},
	month = jan,
	year = {2010},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/5ZRT34N2/Fiebrink and Cook - 2010 - The Wekinator A System for Real-time, Interactive.pdf:application/pdf},
}

@article{murray-browne_emergent_2021,
	title = {Emergent {Interfaces}: {Vague}, {Complex}, {Bespoke} and {Embodied} {Interaction} between {Humans} and {Computers}},
	volume = {11},
	issn = {2076-3417},
	shorttitle = {Emergent {Interfaces}},
	url = {https://www.mdpi.com/2076-3417/11/18/8531},
	doi = {10.3390/app11188531},
	abstract = {Most Human–Computer Interfaces are built on the paradigm of manipulating abstract representations. This can be limiting when computers are used in artistic performance or as mediators of social connection, where we rely on qualities of embodied thinking: intuition, context, resonance, ambiguity and ﬂuidity. We explore an alternative approach to designing interaction that we call the emergent interface: interaction leveraging unsupervised machine learning to replace designed abstractions with contextually derived emergent representations. The approach offers opportunities to create interfaces bespoke to a single individual, to continually evolve and adapt the interface in line with that individual’s needs and affordances, and to bridge more deeply with the complex and imprecise interaction that deﬁnes much of our non-digital communication. We explore this approach through artistic research rooted in music, dance and AI with the partially emergent system Soniﬁed Body. The system maps the moving body into sound using an emergent representation of the body derived from a corpus of improvised movement from the ﬁrst author. We explore this system in a residency with three dancers. We reﬂect on the broader implications and challenges of this alternative way of thinking about interaction, and how far it may help users avoid being limited by the assumptions of a system’s designer.},
	language = {en},
	number = {18},
	urldate = {2021-11-27},
	journal = {Applied Sciences},
	author = {Murray-Browne, Tim and Tigas, Panagiotis},
	month = sep,
	year = {2021},
	pages = {8531},
	file = {Murray-Browne and Tigas - 2021 - Emergent Interfaces Vague, Complex, Bespoke and E.pdf:/Users/admin 1/Zotero/storage/I98QTZ38/Murray-Browne and Tigas - 2021 - Emergent Interfaces Vague, Complex, Bespoke and E.pdf:application/pdf},
}

@misc{noauthor_whosampled_nodate,
	title = {{WhoSampled}},
	url = {https://www.whosampled.com/},
	abstract = {Dig deeper into music at the ultimate database of sampled music, cover songs and remixes},
	language = {en},
	urldate = {2021-12-14},
}

@incollection{exarchos_boom_2019,
	address = {New York},
	title = {Boom bap ex machina: hip-hop aesthetics and the {Akai} {MPC}},
	isbn = {978-0-415-78921-9},
	shorttitle = {Boom bap ex machina},
	url = {http://repository.uwl.ac.uk/id/eprint/5150/},
	abstract = {Over the past two decades, the growing literature on Hip-Hop musicology has paid ample tribute to Akai’s range of MPCs (Music Production Controllers), acknowledging their pivotal influence on the methodologies of Rap production. The technology’s combined functionality of sampling, drum-programming and MIDI-sequencing has been embraced by practitioners since the release of the standalone MPC60 in 1988, until its more recent computer-dependent incarnations manifested in a multitude of current controllers and DAWs. The timeline coincides with particular sonic priorities in Hip-Hop that can be grouped under the ‘Boom Bap’ aesthetic - an onomatopoeic celebration of the prominence of sampled drum sounds programmed over sparse and heavily syncopated instrumentation. But what is the association between sub-genre aesthetics and MPC functionality, and what parallels can be drawn between the evolution of the technology and stylistic deviations in the genre? The chapter examines how MPC technology impacts upon the stylisation of Hip-Hop as a result of unique sonic, rhythmic and user-interface characteristics, which condition sampling, programming and mixing practices, determining recognisable sonic signatures. Furthermore, the Boom Bap sound is traced from its origins in the late 1980s to its current use as an East Coast production reference, honouring a sample-based philosophy that is facilitated by the MPCs’ physical interface and operating-system scripts. Utilising a two-fold methodology that combines musicological analysis with practice-led actualisation, the research explores a number of representative case studies, before investigating the interactivity of original composition and MPC workflow further. The findings form a systematic typology of technical characteristics correlated to creative approaches and resulting production traits, informing speculation on the future of the MPC, its technological descendants and the footprint of its aesthetic on emerging styles and technologies. 

Keywords: Hip-Hop, Aesthetics, MPC, Sampling, Boom Bap, Production},
	language = {en},
	urldate = {2022-01-25},
	publisher = {Routledge},
	author = {Exarchos, Michail},
	editor = {Hepworth-Sawyer, Russ and Hodgson, Jay and Marrington, Mark},
	month = mar,
	year = {2019},
	note = {Num Pages: 26},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/LP4N9PKM/Exarchos - 2019 - Boom bap ex machina hip-hop aesthetics and the Ak.pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/T5VR5X8L/5150.html:text/html},
}

@misc{abraham_visualizing_2016,
	title = {Visualizing {Neural} {Networks} {In} {Virtual} {Space}},
	url = {https://medium.com/dbrs-innovation-labs/visualizing-neural-networks-in-virtual-space-7e3f62f7177},
	abstract = {“You guys may be surprised to hear this, but I didn’t really get into tech until college.”},
	language = {en},
	urldate = {2022-08-09},
	journal = {DBRS Innovation Labs},
	author = {Abraham, Eamon},
	month = apr,
	year = {2016},
	file = {Snapshot:/Users/admin 1/Zotero/storage/74Z47IE3/visualizing-neural-networks-in-virtual-space-7e3f62f7177.html:text/html},
}

@misc{puckette_real-time_1998,
	title = {Real-time audio analysis tools for {Pd} and {MSP}},
	abstract = {Two "objects," which run under Max/MSP or Pd, do different kinds of real-time analysis of musical sounds. Fiddle is a monophonic or polyphonic maximum-likelihood pitch detector similar to Rabiner's, which can also be used to obtain a raw list of a signal's sinusoidal components. Bonk  does a bounded-Q analysis of an incoming sound to detect onsets of percussion instruments in a way which outperforms the standard envelope following technique. The outputs of both objects appear as Max-style control messages. 1 Tools for real-time audio analysis  The new real-time patchable software synthesizers have finally brought audio signal processing out of the ivory tower and into the homes of working computer musicians. Now audio can be placed at the center of real-time computer music production, and MIDI, which for a decade was the backbone of the electronic music studio, can be relegated to its appropriate role as a low-bandwidth I/O solution for keyboards and other input devices. Many other sou...},
	author = {Puckette, Miller S. and Ucsd, Miller S. Puckette and Apel, Theodore and Edu, Ucsd (tapel@ucsd and Zicarelli, David D. and Com, Cycling (www Cycling},
	year = {1998},
	file = {Citeseer - Full Text PDF:/Users/admin 1/Zotero/storage/EUVM6YQV/Puckette et al. - 1998 - Real-time audio analysis tools for Pd and MSP.pdf:application/pdf;Citeseer - Snapshot:/Users/admin 1/Zotero/storage/XMDSDSCL/summary.html:text/html},
}

@inproceedings{bryan-kinns_exploring_2021,
	title = {Exploring {XAI} for the {Arts}: {Explaining} {Latent} {Space} in {Generative} {Music}},
	shorttitle = {Exploring {XAI} for the {Arts}},
	url = {https://openreview.net/forum?id=GLhY_0xMLZr},
	abstract = {As a first step in exploring Explainable AI for the arts we demonstrate how a latent variable model, specifically MeasureVAE which generates measures of music, can be made more explainable.},
	language = {en},
	urldate = {2022-10-11},
	author = {Bryan-Kinns, Nick and Banar, Berker and Ford, Corey and Reed, Courtney N. and Zhang, Yixiao and Colton, Simon and Armitage, Jack},
	month = nov,
	year = {2021},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/BEXBFCYU/Bryan-Kinns et al. - 2021 - Exploring XAI for the Arts Explaining Latent Spac.pdf:application/pdf},
}

@article{hadjeres_deepbach_2017,
	title = {{DeepBach}: a {Steerable} {Model} for {Bach} {Chorales} {Generation}},
	abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and speciﬁcally hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
	language = {en},
	journal = {Proceedings of the 34th International Conference on Machine Learning},
	author = {Hadjeres, Gaëtan and Pachet, François and Nielsen, Frank},
	year = {2017},
	pages = {10},
	file = {Hadjeres et al. - DeepBach a Steerable Model for Bach Chorales Gene.pdf:/Users/admin 1/Zotero/storage/ACLFI8W2/Hadjeres et al. - DeepBach a Steerable Model for Bach Chorales Gene.pdf:application/pdf},
}

@misc{tang_what_2022,
	title = {What the {DAAM}: {Interpreting} {Stable} {Diffusion} {Using} {Cross} {Attention}},
	shorttitle = {What the {DAAM}},
	url = {http://arxiv.org/abs/2210.04885},
	doi = {10.48550/arXiv.2210.04885},
	abstract = {Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, with some performing similar to real photographs in human evaluation. However, they remain poorly understood, lacking explainability and interpretability analyses, largely due to their proprietary, closed-source nature. In this paper, to shine some much-needed light on text-to-image diffusion models, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced large diffusion model. To produce pixel-level attribution maps, we propose DAAM, a novel method based on upscaling and aggregating cross-attention activations in the latent denoising subnetwork. We support its correctness by evaluating its unsupervised semantic segmentation quality on its own generated imagery, compared to supervised segmentation models. We show that DAAM performs strongly on COCO caption-generated images, achieving an mIoU of 61.0, and it outperforms supervised models on open-vocabulary segmentation, for an mIoU of 51.5. We further find that certain parts of speech, like punctuation and conjunctions, influence the generated imagery most, which agrees with the prior literature, while determiners and numerals the least, suggesting poor numeracy. To our knowledge, we are the first to propose and study word-pixel attribution for interpreting large-scale diffusion models. Our code and data are at https://github.com/castorini/daam.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Tang, Raphael and Pandey, Akshat and Jiang, Zhiying and Yang, Gefei and Kumar, Karun and Lin, Jimmy and Ture, Ferhan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.04885 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Generative Explanations},
	file = {arXiv.org Snapshot:/Users/admin 1/Zotero/storage/Z92AFB55/2210.html:text/html;Tang et al. - 2022 - What the DAAM Interpreting Stable Diffusion Using.pdf:/Users/admin 1/Zotero/storage/N8RHA54C/Tang et al. - 2022 - What the DAAM Interpreting Stable Diffusion Using.pdf:application/pdf},
}

@article{wachter_right_2019,
	title = {A {Right} to {Reasonable} {Inferences}: {Re}-{Thinking} {Data} {Protection} {Law} in the {Age} of {Big} {Data} and {AI}},
	volume = {2019},
	copyright = {Copyright (c) 2019 Sandra Wachter and Brent Mittelstadt},
	issn = {0898-0721},
	shorttitle = {A {Right} to {Reasonable} {Inferences}},
	url = {https://journals.library.columbia.edu/index.php/CBLR/article/view/3424},
	doi = {10.7916/cblr.v2019i2.3424},
	abstract = {Big Data analytics and artificial intelligence (AI) draw non-intuitive and unverifiable inferences and predictions about the behaviors, preferences, and private lives of individuals. These inferences draw on highly diverse and feature-rich data of unpredictable value, and create new opportunities for discriminatory, biased, and invasive decision-making. Data protection law is meant to protect people’s privacy, identity, reputation, and autonomy, but is currently failing to protect data subjects from the novel risks of inferential analytics. The legal status of inferences is heavily disputed in legal scholarship, and marked by inconsistencies and contradictions within and between the views of the Article 29 Working Party and the European Court of Justice (ECJ).
This Article shows that individuals are granted little control or oversight over how their personal data is used to draw inferences about them. Compared to other types of personal data, inferences are effectively “economy class” personal data in the General Data Protection Regulation (GDPR). Data subjects’ rights to know about (Articles 13–15),\&nbsp;rectify (Article 16), delete (Article 17), object to (Article 21), or port (Article 20) personal data are significantly curtailed for inferences. The GDPR also provides insufficient protection against sensitive inferences (Article 9) or remedies to challenge inferences or important decisions based on them (Article 22(3)).
This situation is not accidental. In standing jurisprudence the ECJ has consistently restricted the remit of data protection law to assessing the legitimacy of input personal data undergoing processing, and to rectify, block, or erase it. Critically, the ECJ has likewise made clear that data protection law is not intended to ensure the accuracy of decisions and decision-making processes involving personal data, or to make these processes fully transparent. Current policy proposals addressing privacy protection (the ePrivacy Regulation and the EU Digital Content Directive) and Europe’s new Copyright Directive and Trade Secrets Directive also fail to close the GDPR’s accountability gaps concerning inferences.
This Article argues that a new data protection right, the “right to reasonable inferences,” is needed to help close the accountability gap currently posed by “high risk inferences,” meaning inferences drawn from Big Data analytics that damage privacy or reputation, or have low verifiability in the sense of being predictive or opinion-based while being used in important decisions. This right would require ex-ante justification to be given by the data controller to establish whether an inference is reasonable. This disclosure would address (1) why certain data form a normatively acceptable basis from which to draw inferences; (2) why these inferences are relevant and normatively acceptable for the chosen processing purpose or type of automated decision; and (3) whether the data and methods used to draw the inferences are accurate and statistically reliable. The ex-ante justification is bolstered by an additional ex-post mechanism enabling unreasonable inferences to be challenged.},
	language = {en},
	number = {2},
	urldate = {2022-10-30},
	journal = {Columbia Business Law Review},
	author = {Wachter, Sandra and Mittelstadt, Brent},
	month = may,
	year = {2019},
	note = {Number: 2},
	pages = {494--620},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/9V5WU5LW/Wachter and Mittelstadt - 2019 - A Right to Reasonable Inferences Re-Thinking Data.pdf:application/pdf},
}

@article{mannes_governance_2020,
	title = {Governance, {Risk}, and {Artificial} {Intelligence}},
	volume = {41},
	copyright = {Copyright (c) 2020 AI Magazine},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/5200},
	doi = {10.1609/aimag.v41i1.5200},
	abstract = {Artificial intelligence, whether embodied as robots or Internet of Things, or disembodied as intelligent agents or decision-support systems, can enrich the human experience. It will also fail and cause harms, including physical injury and financial loss as well as more subtle harms such as instantiating human bias or undermining individual dignity. These failures could have a disproportionate impact because strange, new, and unpredictable dangers may lead to public discomfort and rejection of artificial intelligence. Two possible approaches to mitigating these risks are the hard power of regulating artificial intelligence, to ensure it is safe, and the soft power of risk communication, which engages the public and builds trust. These approaches are complementary and both should be implemented as artificial intelligence becomes increasingly prevalent in daily life.},
	language = {en},
	number = {1},
	urldate = {2022-11-05},
	journal = {AI Magazine},
	author = {Mannes, Aaron},
	month = apr,
	year = {2020},
	note = {Number: 1},
	pages = {61--69},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/TZXSK958/Mannes - 2020 - Governance, Risk, and Artificial Intelligence.pdf:application/pdf},
}

@article{lutz_digital_2019,
	title = {Digital inequalities in the age of artificial intelligence and big data},
	volume = {1},
	issn = {2578-1863},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbe2.140},
	doi = {10.1002/hbe2.140},
	abstract = {In this literature review, I summarize key concepts and findings from the rich academic literature on digital inequalities. I propose that digital inequalities research should look more into labor- and big data-related questions such as inequalities in online labor markets and the negative effects of algorithmic decision-making for vulnerable population groups. The article engages with the sociological literature on digital inequalities and explains the general approach to digital inequalities, based on the distinction of first-, second-, and third-level digital divides. First, inequalities in access to digital technologies are discussed. This discussion is extended to emerging technologies, including the Internet-of-things and artificial intelligence-powered systems such as smart speakers. Second, inequalities in digital skills and technology use are reviewed and connected to the discourse on new forms of work such as the sharing economy or gig economy. Third and finally, the discourse on the outcomes, in the form of benefits or harms, from digital technology use is taken up. Here, I propose to integrate the digital inequalities literature more strongly with critical algorithm studies and recent discussions about datafication, digital footprints, and information privacy.},
	language = {en},
	number = {2},
	urldate = {2022-11-05},
	journal = {Human Behavior and Emerging Technologies},
	author = {Lutz, Christoph},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbe2.140},
	keywords = {AI, digital divide, privacy},
	pages = {141--148},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/AEL3MIID/Lutz - 2019 - Digital inequalities in the age of artificial inte.pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/FWJYHM2S/hbe2.html:text/html},
}

@misc{crawford_trouble_2017,
	title = {The {Trouble} {With} {Bias} {NIPS} 2017 {Keynote} {Kate} {Crawford}},
	url = {http://archive.org/details/the-trouble-with-bias-nips-2017-keynote-kate-crawford-nips-2017-f-mym-bkwqzk},
	abstract = {Kate Crawford is a leading researcher, academic and author who has spent
 the last decade studying the social implications of data systems, 
machine learning and artificial intelligence. She is a Distinguished 
Research Professor at New York University, a Principal Researcher at 
Microsoft Research New York, and a Visiting Professor at the MIT Media 
Lab.},
	language = {eng},
	urldate = {2022-11-05},
	author = {Crawford, Kate},
	month = dec,
	year = {2017},
	keywords = {Bias in AI},
}

@article{mahoney_how_2020,
	title = {How to {Measure} and {Reduce} {Unwanted} {Bias} in {Machine} {Learning}},
	language = {en},
	author = {Mahoney, Trisha and Varshney, Kush R and Hind, Michael},
	year = {2020},
	pages = {35},
	file = {Mahoney et al. - How to Measure and Reduce Unwanted Bias in Machine.pdf:/Users/admin 1/Zotero/storage/5JKETR8R/Mahoney et al. - How to Measure and Reduce Unwanted Bias in Machine.pdf:application/pdf},
}

@incollection{dastin_amazon_2022,
	title = {Amazon {Scraps} {Secret} {AI} {Recruiting} {Tool} that {Showed} {Bias} against {Women} *},
	isbn = {978-1-00-327829-0},
	abstract = {Automation has been key to Amazon's e-commerce dominance, be it inside warehouses or driving pricing decisions. The company's experimental hiring tool used artificial intelligence to give job candidates scores ranging from one to five stars - much like shoppers rate products on Amazon. In effect, Amazon's system taught itself that male candidates were preferable. It penalized resumes that included the word “women's,” as in “women's chess club captain.” And it downgraded graduates of two all-women's colleges, according to people familiar with the matter. Amazon's experiment began at a pivotal moment for the world's largest online retailer. Machine learning was gaining traction in the technology world. The American civil liberties union is currently challenging a law that allows criminal prosecution of researchers and journalists who test hiring websites' algorithms for discrimination.},
	booktitle = {Ethics of {Data} and {Analytics}},
	publisher = {Auerbach Publications},
	author = {Dastin, Jeffrey},
	year = {2022},
	note = {Num Pages: 4},
}

@article{yarger_algorithmic_2019,
	title = {Algorithmic equity in the hiring of underrepresented {IT} job candidates},
	volume = {44},
	issn = {1468-4527},
	url = {https://doi.org/10.1108/OIR-10-2018-0334},
	doi = {10.1108/OIR-10-2018-0334},
	abstract = {Purpose The purpose of this paper is to offer a critical analysis of talent acquisition software and its potential for fostering equity in the hiring process for underrepresented IT professionals. The under-representation of women, African-American and Latinx professionals in the IT workforce is a longstanding issue that contributes to and is impacted by algorithmic bias. Design/methodology/approach Sources of algorithmic bias in talent acquisition software are presented. Feminist design thinking is presented as a theoretical lens for mitigating algorithmic bias. Findings Data are just one tool for recruiters to use; human expertise is still necessary. Even well-intentioned algorithms are not neutral and should be audited for morally and legally unacceptable decisions. Feminist design thinking provides a theoretical framework for considering equity in the hiring decisions made by talent acquisition systems and their users. Social implications This research implies that algorithms may serve to codify deep-seated biases, making IT work environments just as homogeneous as they are currently. If bias exists in talent acquisition software, the potential for propagating inequity and harm is far more significant and widespread due to the homogeneity of the specialists creating artificial intelligence (AI) systems. Originality/value This work uses equity as a central concept for considering algorithmic bias in talent acquisition. Feminist design thinking provides a framework for fostering a richer understanding of what fairness means and evaluating how AI software might impact marginalized populations.},
	number = {2},
	urldate = {2022-11-10},
	journal = {Online Information Review},
	author = {Yarger, Lynette and Cobb Payton, Fay and Neupane, Bikalpa},
	month = jan,
	year = {2019},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Equity, Talent acquisition},
	pages = {383--395},
	file = {Snapshot:/Users/admin 1/Zotero/storage/2AFDQD7F/html.html:text/html},
}

@article{zavrsnik_criminal_2020,
	title = {Criminal justice, artificial intelligence systems, and human rights},
	volume = {20},
	issn = {1863-9038},
	url = {https://doi.org/10.1007/s12027-020-00602-0},
	doi = {10.1007/s12027-020-00602-0},
	abstract = {The automation brought about by big data analytics, machine learning and artificial intelligence systems challenges us to reconsider fundamental questions of criminal justice. The article outlines the automation which has taken place in the criminal justice domain and answers the question of what is being automated and who is being replaced thereby. It then analyses encounters between artificial intelligence systems and the law, by considering case law and by analysing some of the human rights affected. The article concludes by offering some thoughts on proposed solutions for remedying the risks posed by artificial intelligence systems in the criminal justice domain.},
	language = {en},
	number = {4},
	urldate = {2022-11-10},
	journal = {ERA Forum},
	author = {Završnik, Aleš},
	month = mar,
	year = {2020},
	keywords = {Algorithms, Artificial intelligence, Automation, Criminal justice, Fair trial, Human rights},
	pages = {567--583},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/Z5LGA8NL/Završnik - 2020 - Criminal justice, artificial intelligence systems,.pdf:application/pdf},
}

@article{freeman_algorithmic_nodate,
	title = {Algorithmic {Injustice}: {How} the {Wisconsin} {Supreme} {Court} {Failed} to {Protect} {Due} {Process} {Rights} in {State} v. {Loomis}},
	volume = {18},
	language = {en},
	author = {Freeman, Katherine},
	pages = {33},
	file = {Freeman - Algorithmic Injustice How the Wisconsin Supreme C.pdf:/Users/admin 1/Zotero/storage/T4EMNPQU/Freeman - Algorithmic Injustice How the Wisconsin Supreme C.pdf:application/pdf},
}

@misc{noauthor_googles_nodate,
	title = {Google's solution to accidental algorithmic racism: ban gorillas {\textbar} {Google} {\textbar} {The} {Guardian}},
	url = {https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people},
	urldate = {2022-11-10},
	file = {Google's solution to accidental algorithmic racism\: ban gorillas | Google | The Guardian:/Users/admin 1/Zotero/storage/5Y25D4DV/google-racism-ban-gorilla-black-people.html:text/html},
}

@article{prates_assessing_2020,
	title = {Assessing gender bias in machine translation: a case study with {Google} {Translate}},
	volume = {32},
	issn = {1433-3058},
	shorttitle = {Assessing gender bias in machine translation},
	url = {https://doi.org/10.1007/s00521-019-04144-6},
	doi = {10.1007/s00521-019-04144-6},
	abstract = {Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias, where trained statistical models—unbeknownst to their creators—grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple’s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos’ mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI.In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like “He/She is an Engineer” (where “Engineer” is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output.We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS’ data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques—which can already be found in the scientific literature.},
	language = {en},
	number = {10},
	urldate = {2022-11-10},
	journal = {Neural Computing and Applications},
	author = {Prates, Marcelo O. R. and Avelar, Pedro H. and Lamb, Luís C.},
	month = may,
	year = {2020},
	keywords = {Gender bias, Machine bias, Machine learning, Machine translation},
	pages = {6363--6381},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/ECWPZHR4/Prates et al. - 2020 - Assessing gender bias in machine translation a ca.pdf:application/pdf},
}

@misc{noauthor_evolution_nodate,
	title = {The {Evolution} of {Artificial} {Intelligence} ({AI}) and its impact on {Women}: how it nurtures discriminations towards women and strengthens gender inequality},
	shorttitle = {The {Evolution} of {Artificial} {Intelligence} ({AI}) and its impact on {Women}},
	url = {http://www.humanrights.periodikos.com.br/article/61489565a9539526b5418543},
	abstract = {Arribat, vol.1, n2, p.141-150, 2021},
	language = {en, pt-br},
	urldate = {2022-11-10},
	journal = {Periodikos},
	file = {Snapshot:/Users/admin 1/Zotero/storage/XDNJM5FT/61489565a9539526b5418543.html:text/html},
}

@article{theunissen_putting_2022,
	title = {Putting explainable {AI} in context: institutional explanations for medical {AI}},
	volume = {24},
	issn = {1572-8439},
	shorttitle = {Putting explainable {AI} in context},
	url = {https://doi.org/10.1007/s10676-022-09649-8},
	doi = {10.1007/s10676-022-09649-8},
	abstract = {There is a current debate about if, and in what sense, machine learning systems used in the medical context need to be explainable. Those arguing in favor contend these systems require post hoc explanations for each individual decision to increase trust and ensure accurate diagnoses. Those arguing against suggest the high accuracy and reliability of the systems is sufficient for providing epistemic justified beliefs without the need for explaining each individual decision. But, as we show, both solutions have limitations—and it is unclear either address the epistemic worries of the medical professionals using these systems. We argue these systems do require an explanation, but an institutional explanation. These types of explanations provide the reasons why the medical professional should rely on the system in practice—that is, they focus on trying to address the epistemic concerns of those using the system in specific contexts and specific occasions. But ensuring that these institutional explanations are fit for purpose means ensuring the institutions designing and deploying these systems are transparent about the assumptions baked into the system. This requires coordination with experts and end-users concerning how it will function in the field, the metrics used to evaluate its accuracy, and the procedures for auditing the system to prevent biases and failures from going unaddressed. We contend this broader explanation is necessary for either post hoc explanations or accuracy scores to be epistemically meaningful to the medical professional, making it possible for them to rely on these systems as effective and useful tools in their practices.},
	language = {en},
	number = {2},
	urldate = {2022-11-11},
	journal = {Ethics and Information Technology},
	author = {Theunissen, Mark and Browning, Jacob},
	month = may,
	year = {2022},
	keywords = {AI and health, Epistemic risk, Ethical design, Explainable AI},
	pages = {23},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/QKWAJCT2/Theunissen and Browning - 2022 - Putting explainable AI in context institutional e.pdf:application/pdf},
}

@article{simonite_when_nodate,
	title = {When {It} {Comes} to {Gorillas}, {Google} {Photos} {Remains} {Blind}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/},
	abstract = {Google promised a fix after its photo-categorization software labeled black people as gorillas in 2015. More than two years later, it hasn't found one.},
	language = {en-US},
	urldate = {2022-11-11},
	journal = {Wired},
	author = {Simonite, Tom},
	note = {Section: tags},
	file = {Snapshot:/Users/admin 1/Zotero/storage/BTR8DHLG/when-it-comes-to-gorillas-google-photos-remains-blind.html:text/html},
}

@inproceedings{puche_caesynth_2021,
	title = {Caesynth: {Real}-{Time} {Timbre} {Interpolation} and {Pitch} {Control} with {Conditional} {Autoencoders}},
	shorttitle = {Caesynth},
	doi = {10.1109/MLSP52302.2021.9596414},
	abstract = {In this paper, we present a novel audio synthesizer, CAESynth, based on a conditional autoencoder. CAESynth synthesizes timbre in real-time by interpolating the reference sounds in their shared latent feature space, while controlling a pitch independently. We show that training a conditional autoen-coder based on accuracy in timbre classification together with adversarial regularization of pitch content allows timbre distribution in latent space to be more effective and stable for timbre interpolation and pitch conditioning. The proposed method is applicable not only to creation of musical cues but also to exploration of audio affordance in mixed reality based on novel timbre mixtures with environmental sounds. We demonstrate by experiments that CAESynth achieves smooth and high-fidelity audio synthesis in real-time through timbre interpolation and independent yet accurate pitch control for musical cues as well as for audio affordance with environmental sound. A Python implementation along with some generated samples are shared online.},
	booktitle = {2021 {IEEE} 31st {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Puche, Aaron Valero and Lee, Sukhan},
	month = oct,
	year = {2021},
	note = {ISSN: 1551-2541},
	keywords = {Aerospace electronics, Affordances, Audio Mixed Reality, Audio Synthesis, Autoencoders, Disentanglement, Interpolation, Machine learning, Mixed reality, Pitch control (audio), Timbre Interpolation, Training},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/admin 1/Zotero/storage/P33GVNDN/9596414.html:text/html;Submitted Version:/Users/admin 1/Zotero/storage/NUFIRM7C/Puche and Lee - 2021 - Caesynth Real-Time Timbre Interpolation and Pitch.pdf:application/pdf},
}

@article{tatar_latent_2021,
	title = {Latent {Timbre} {Synthesis}},
	volume = {33},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-020-05424-2},
	doi = {10.1007/s00521-020-05424-2},
	abstract = {We present the Latent Timbre Synthesis, a new audio synthesis method using deep learning. The synthesis method allows composers and sound designers to interpolate and extrapolate between the timbre of multiple sounds using the latent space of audio frames. We provide the details of two Variational Autoencoder architectures for the Latent Timbre Synthesis and compare their advantages and drawbacks. The implementation includes a fully working application with a graphical user interface, called interpolate\_two, which enables practitioners to generate timbres between two audio excerpts of their selection using interpolation and extrapolation in the latent space of audio frames. Our implementation is open source, and we aim to improve the accessibility of this technology by providing a guide for users with any technical background. Our study includes a qualitative analysis where nine composers evaluated the Latent Timbre Synthesis and the interpolate\_two application within their practices.},
	language = {en},
	number = {1},
	urldate = {2022-11-13},
	journal = {Neural Computing and Applications},
	author = {Tatar, Kıvanç and Bisig, Daniel and Pasquier, Philippe},
	month = jan,
	year = {2021},
	keywords = {Audio synthesis, Computer assisted music composition, Neural networks, Signal processing},
	pages = {67--84},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/TSRE82KK/Tatar et al. - 2021 - Latent Timbre Synthesis.pdf:application/pdf},
}

@inproceedings{colonel_conditioning_2020,
	title = {Conditioning {Autoencoder} {Latent} {Spaces} for {Real}-{Time} {Timbre} {Interpolation} and {Synthesis}},
	doi = {10.1109/IJCNN48605.2020.9207666},
	abstract = {We compare standard autoencoder topologies' performances for timbre generation. We demonstrate how different activation functions used in the autoencoder's bottleneck distributes a training corpus's embedding. We show that the choice of sigmoid activation in the bottleneck produces a more bounded and uniformly distributed embedding than a leaky rectified linear unit activation. We propose a one-hot encoded chroma feature vector for use in both input augmentation and latent space conditioning. We measure the performance of these networks, and characterize the latent embeddings that arise from the use of this chroma conditioning vector. An open source, real-time timbre synthesis algorithm in Python is outlined and shared.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Colonel, Joseph T and Keene, Sam},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {autoencoder, Decoding, neural network, Neurons, real-time audio, Testing, Timbre, timbre synthesis, Topology, Training},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:/Users/admin 1/Zotero/storage/H9PQ5D76/9207666.html:text/html;Submitted Version:/Users/admin 1/Zotero/storage/XPS2KFDP/Colonel and Keene - 2020 - Conditioning Autoencoder Latent Spaces for Real-Ti.pdf:application/pdf},
}

@article{cornock_creative_1973,
	title = {The {Creative} {Process} {Where} the {Artist} {Is} {Amplified} or {Superseded} by the {Computer}},
	volume = {6},
	issn = {0024-094X},
	url = {https://www.jstor.org/stable/1572419},
	doi = {10.2307/1572419},
	abstract = {The advent of computing stimulates a desire to re-examine the subject of creativity. Though the computer can replace man in the production of graphic images, its function in the arts is seen as assisting in the specification of art systems and in their subsequent real-time management. An art of system or process is placed in the context of primarily the visual or plastic arts but the authors disavow concern with any 'new' or 'modern technological' art. Various types of art systems are mentioned and advantages of the fully interactive one are considered. It is pointed out that the inclusion of complex real-time responses in an interactive art system can frequently make use of a computer. In such work, the artist and the viewer play an integral part. The traditional role of the artist, composer or writer is thus called into question; it may no longer be necessary to assume that he is a specialist in art--rather he is a catalyst of creative activity. Three cases are discussed to illustrate the applications of this approach. /// L'avènement de l'informatique exige que l'on reconsidère le problème de la créativité. Bien que l'ordinateur puisse remplacer l'homme dans la production d'images graphiques, on peut penser que sa fonction dans les arts est d'aider à spécifier les systèmes artistiques, et à les répartir ultérieurement dans le temps réel. Un art du système et du processus se situe d'abord dans le contexte des arts visuels ou plastiques; cependant la préoccupation des auteurs n'est d'aucune manière un art 'nouveau' ou 'technologique'. Ils citent plusieurs sortes de systèmes artistiques et décrivent les avantages de celui qui est entièrement 'interactif'. Ils font remarquer que l'inclusion de réponses complexes du temps réel dans un système artistique interactif peut souvent appeler l'utilisation d'un ordinateur. Dans une telle œuvre, l'artiste et le spectateur jouent un rôle à part entière. Le rôle traditionnel de l'artiste, du compositeur ou de l'écrivain est ainsi remis en question; il n'est plus nécessaire d'affirmer que l'artiste est un spécialiste--il est plutôt le catalyseur d'une activité créatrice. Trois exemples illustrent les applications de cette approche.},
	number = {1},
	urldate = {2022-11-13},
	journal = {Leonardo},
	author = {Cornock, Stroud and Edmonds, Ernest},
	year = {1973},
	note = {Publisher: The MIT Press},
	pages = {11--16},
}

@book{dinculescu_midime_2019,
	title = {{MidiMe}: {Personalizing} a {MusicVAE} model with user data},
	shorttitle = {{MidiMe}},
	editor = {Dinculescu, Monica and Engel, Jesse and Roberts, Adam},
	year = {2019},
	note = {Publication Title: Workshop on Machine Learning for Creativity and Design, NeurIPS},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/UIPUBSCJ/Dinculescu et al. - 2019 - MidiMe Personalizing a MusicVAE model with user d.pdf:application/pdf},
}

@article{xambo_sedo_live_2018,
	title = {Live {Repurposing} of {Sounds}: {MIR} {Explorations} with {Personal} and {Crowdsourced} {Databases}},
	copyright = {Navngivelse 4.0 Internasjonal},
	shorttitle = {Live {Repurposing} of {Sounds}},
	url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2611596},
	abstract = {The recent increase in the accessibility and size of personal and crowdsourced digital sound collections brought about a valuable resource for music creation. Finding and retrieving relevant sounds in performance leads to challenges that can be approached using music information retrieval (MIR). In this paper, we explore the use of MIR to retrieve and repurpose sounds in musical live coding. We present a live coding system built on SuperCollider enabling the use of audio content from online Creative Commons (CC) sound databases such as Freesound or personal sound databases. The novelty of our approach lies in exploiting high-level MIR methods (e.g., query by pitch or rhythmic cues) using live coding techniques applied to sounds. We demonstrate its potential through the reflection of an illustrative case study and the feedback from four expert users. The users tried the system with either a personal database or a crowdsourced database and reported its potential in facilitating tailorability of the tool to their own creative workflows.},
	language = {eng},
	urldate = {2023-04-24},
	journal = {364-369},
	author = {Xambo Sedo, Anna and Roma, Gerard and Lerch, Alexander and Barthet, Mathieu and Fazekas, György},
	year = {2018},
	note = {Accepted: 2019-08-29T11:18:55Z
Publisher: Virginia Tech Blacksburg, VA},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/KYXZTJ7S/Xambo Sedo et al. - 2018 - Live Repurposing of Sounds MIR Explorations with .pdf:application/pdf},
}

@misc{clarice_hilton_figural_2023,
	title = {Figural {Bodies}},
	url = {https://www.mocapstreamer.live/figural-bodies},
	author = {{Clarice Hilton} and {Kat Hawkins} and {Susanna Dye} and {Ashley Noel-Hirst} and {Neal Coghlan}},
	year = {2023},
}

@misc{noauthor_meta-design_nodate,
	title = {Meta-design: a manifesto for end-user development: {Communications} of the {ACM}: {Vol} 47, {No} 9},
	url = {https://dl.acm.org/doi/abs/10.1145/1015864.1015884?casa_token=IDLIertBWZEAAAAA:6J6WBVD_UYsyqEoA-TmYSFhqGo5cmD9bn6TdxM9LX5BWizTX1x1iraPVY6RWuv5d7f83qYM45HO3Xg},
	urldate = {2023-05-15},
	file = {Meta-design\: a manifesto for end-user development\: Communications of the ACM\: Vol 47, No 9:/Users/admin 1/Zotero/storage/7R9FVQJN/1015864.html:text/html},
}

@book{lieberman_end_2006,
	address = {Dordrecht},
	edition = {1.ed., 2. printing},
	series = {Human-computer interaction series},
	title = {End user development},
	isbn = {978-1-4020-5309-2 978-1-4020-4220-1},
	language = {en},
	number = {9},
	publisher = {Springer},
	editor = {Lieberman, Henry and Paternò, Fabio and Wulf, Volker},
	year = {2006},
	file = {Lieberman et al. - 2006 - End user development.pdf:/Users/admin 1/Zotero/storage/WL3IKWTY/Lieberman et al. - 2006 - End user development.pdf:application/pdf},
}

@article{barricelli_end-user_2019,
	title = {End-user development, end-user programming and end-user software engineering: {A} systematic mapping study},
	volume = {149},
	issn = {0164-1212},
	shorttitle = {End-user development, end-user programming and end-user software engineering},
	url = {https://www.sciencedirect.com/science/article/pii/S0164121218302577},
	doi = {10.1016/j.jss.2018.11.041},
	abstract = {End-User Development (EUD), End-Programming (EUP) and End-User Software Engineering (EUSE) are three related research fields that study methods and techniques for empowering end users to modify and create digital artifacts. This paper presents a systematic mapping study aimed at identifying and classifying scientific literature about EUD, EUP and EUSE in the time range January 2000–May 2017. We selected 165 papers found through a manual selection of papers from specific conferences, journal special issues, and books, integrated with an automatic search on the most important digital libraries. The answer to our research question was built through a classification of the selected papers on seven dimensions: type of approach, interaction technique, phase in which the approach is adopted, application domain, target use, class of users, and type of evaluation. Our findings suggest that EUD, EUP and EUSE are active research topics not only in Human–Computer Interaction, but also in other research communities. However, little cross-fertilization exists among the three themes, as well as unifying frameworks and approaches for guiding novice designers and practitioners. Other findings highlight trends and gaps related to the analysis’ dimensions, which have implications on the design of future tools and suggest open issues for further investigations.},
	language = {en},
	urldate = {2023-05-15},
	journal = {Journal of Systems and Software},
	author = {Barricelli, Barbara Rita and Cassano, Fabio and Fogli, Daniela and Piccinno, Antonio},
	month = mar,
	year = {2019},
	keywords = {End-user development, End-user programming, End-user software engineering, Systematic mapping study},
	pages = {101--137},
	file = {ScienceDirect Snapshot:/Users/admin 1/Zotero/storage/CS8IFNY4/S0164121218302577.html:text/html},
}

@book{noauthor_end-user_nodate,
	title = {End-{User} {Development}},
	url = {https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/end-user-development},
	abstract = {Authoritative overview of End-User Development (EUD) including 4 HD video interviews filmed in Rome, Italy. EUD is really all about democratization of computing.},
	language = {en},
	urldate = {2023-05-15},
	file = {Snapshot:/Users/admin 1/Zotero/storage/WGI4292M/end-user-development.html:text/html},
}

@article{cohen_ethnography_1993,
	title = {Ethnography and {Popular} {Music} {Studies}},
	volume = {12},
	issn = {0261-1430},
	url = {https://www.jstor.org/stable/931294},
	number = {2},
	urldate = {2023-05-17},
	journal = {Popular Music},
	author = {Cohen, Sara},
	year = {1993},
	note = {Publisher: Cambridge University Press},
	pages = {123--138},
	file = {JSTOR Full Text PDF:/Users/admin 1/Zotero/storage/CGDYMTKB/Cohen - 1993 - Ethnography and Popular Music Studies.pdf:application/pdf},
}

@article{cohen_ethnography_1993-1,
	title = {Ethnography and {Popular} {Music} {Studies}},
	volume = {12},
	issn = {0261-1430},
	url = {https://www.jstor.org/stable/931294},
	number = {2},
	urldate = {2023-05-17},
	journal = {Popular Music},
	author = {Cohen, Sara},
	year = {1993},
	note = {Publisher: Cambridge University Press},
	pages = {123--138},
	file = {JSTOR Full Text PDF:/Users/admin 1/Zotero/storage/72A22TX7/Cohen - 1993 - Ethnography and Popular Music Studies.pdf:application/pdf},
}

@inproceedings{myers_invited_2006,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '06},
	title = {Invited research overview: end-user programming},
	isbn = {978-1-59593-298-3},
	shorttitle = {Invited research overview},
	url = {https://dl.acm.org/doi/10.1145/1125451.1125472},
	doi = {10.1145/1125451.1125472},
	abstract = {In the past few decades there has been considerable work on empowering end users to be able to write their own programs, and as a result, users are indeed doing so. In fact, we estimate that over 12 million people in American workplaces would say that they "do programming" at work, and almost 50 million people use spreadsheets or databases (and therefore may potentially program), compared to only 3 million professional programmers. The "programming" systems used by these end users include spreadsheet systems, web authoring tools, business process authoring tools such as Visual Basic, graphical languages for demonstrating the desired behavior of educational simulations, and even professional languages such as Java. The motivation for end-user programming is to have the computer be useful for each person's specific individual needs. While the empirical study of programming has been an HCI topic since the beginning the field, it is only recently that there has been a focus on the End-User Programmer as a separate class from novices who are assumed to be studying to be professional programmers. Another recent focus is on making end-user programming more reliable, using "End-User Software Engineering." This paper gives a brief summary of some current and past research in the area of End-User Programming.},
	urldate = {2023-05-18},
	booktitle = {{CHI} '06 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Myers, Brad A. and Ko, Amy J. and Burnett, Margaret M.},
	month = apr,
	year = {2006},
	keywords = {empirical studies of programmers (ESP), end-user software engineering, natural programming, programming by demonstration, programming by example, psychology of programming, visual programming},
	pages = {75--80},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/DRRJBMJL/Myers et al. - 2006 - Invited research overview end-user programming.pdf:application/pdf},
}

@book{gitelman_raw_2013,
	address = {Cambridge, Massachusetts ; London, England},
	series = {Infrastructures series},
	title = {"{Raw} data" is an oxymoron},
	isbn = {978-0-262-51828-4},
	language = {en},
	publisher = {The MIT Press},
	editor = {Gitelman, Lisa},
	year = {2013},
	keywords = {Data transmission systems, Data warehousing, Databases, Information theory},
	file = {Gitelman - 2013 - Raw data is an oxymoron.pdf:/Users/admin 1/Zotero/storage/YCFGBWPF/Gitelman - 2013 - Raw data is an oxymoron.pdf:application/pdf},
}

@book{golding_data_2020,
	title = {Data {Loam}: {Sometimes} {Hard}, {Usually} {Soft}. {The} {Future} of {Knowledge} {Systems}},
	isbn = {978-3-11-069784-1},
	shorttitle = {Data {Loam}},
	abstract = {Als Reaktion auf die dominante Wirkkraft und Deutungshoheit des Digitalen vereint Data Loam auf der Basis von Positionen der internationalen zeitgenössischen Kunstpraxis radikale Denkansätze.  Vorbei: das Beharren auf Indexikalität und die instrumentelle Reduktion des Wissens. Stattdessen: eine neue Metrik, die Spiel, Neugier, Experiment und Risiko fordert. Als dringende Antwort auf die stetig wachsende Informationsflut, der Bibliotheken, Suchmaschinen und kulturelle Einrichtungen ausgesetzt sind, werden Ansätze entwickelt, die sinnliche Logik, kausale Durchlässigkeit und neue Formen der Mensch-Maschine-Interaktion anregen und erlauben.  Data Loam beleuchtet die Zukunft von Wissenssystemen in Texten zu künstlicher Intelligenz, Kybernetik und Kryptoökonomie: als Gegenmittel zur Zerstreuung apokalyptischer Ängste.},
	language = {en},
	publisher = {Walter de Gruyter GmbH \& Co KG},
	author = {Golding, Johnny and Reinhart, Martin and Paganelli, Mattia},
	month = dec,
	year = {2020},
	note = {Google-Books-ID: mCATEAAAQBAJ},
	keywords = {Philosophy / General, Art / Criticism \& Theory, Art / General, Computers / Artificial Intelligence / Expert Systems, Computers / Data Science / Data Visualization, Language Arts \& Disciplines / Library \& Information Science / General, Philosophy / Epistemology, Philosophy / History \& Surveys / General},
	file = {Data_Loam_Sometimes_Hard_Usually_Soft_The_Future_o.pdf:/Users/admin 1/Zotero/storage/NVHL99VI/Data_Loam_Sometimes_Hard_Usually_Soft_The_Future_o.pdf:application/pdf},
}

@article{horvatic_human-centric_2021,
	title = {Human-{Centric} {AI}: {The} {Symbiosis} of {Human} and {Artificial} {Intelligence}},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {Human-{Centric} {AI}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7998306/},
	doi = {10.3390/e23030332},
	number = {3},
	urldate = {2023-05-22},
	journal = {Entropy},
	author = {Horvatić, Davor and Lipic, Tomislav},
	month = mar,
	year = {2021},
	pmid = {33799841},
	pmcid = {PMC7998306},
	pages = {332},
	file = {PubMed Central Full Text PDF:/Users/admin 1/Zotero/storage/5YH9CEAS/Horvatić and Lipic - 2021 - Human-Centric AI The Symbiosis of Human and Artif.pdf:application/pdf},
}

@inproceedings{gioti_compositional_2021,
	title = {A {Compositional} {Exploration} of {Computational} {Aesthetic} {Evaluation} and {AI} {Bias}},
	doi = {10.21428/92fbeb44.de74b046},
	abstract = {This paper describes a subversive compositional approach to machine learning, focused on the exploration of AI bias and computational aesthetic evaluation. In Bias, for bass clarinet and Interactive Music System, a computer music system using two Neural Networks trained to develop "aesthetic bias" interacts with the musician by evaluating the sound input based on its "subjective" aesthetic judgments. The composition problematizes the discrepancies between the concepts of error and accuracy, associated with supervised machine learning, and aesthetic judgments as inherently subjective and intangible. The methods used in the compositional process are discussed with respect to the objective of balancing the trade-off between musical authorship and interpretative freedom in interactive musical works.},
	author = {Gioti, Artemi - Maria},
	month = jun,
	year = {2021},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/7698EF4F/Gioti - 2021 - A Compositional Exploration of Computational Aesth.pdf:application/pdf},
}

@incollection{gioti_artificial_2021,
	address = {Cham},
	title = {Artificial {Intelligence} for {Music} {Composition}},
	isbn = {978-3-030-72116-9},
	url = {https://doi.org/10.1007/978-3-030-72116-9_3},
	abstract = {This chapter explores the potential of Artificial Intelligence (AI) for art music composition, focusing on how AI can transform musical tasks and shape compositional ideas. In this context, AI is viewed as an agent contributing to a distributed human–computer co-creativity and extending human capabilities, rather than a replacement for human creativity.},
	language = {en},
	urldate = {2023-05-23},
	booktitle = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
	publisher = {Springer International Publishing},
	author = {Gioti, Artemi-Maria},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9_3},
	pages = {53--73},
}

@book{miranda_handbook_2021,
	address = {Cham},
	title = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
	isbn = {978-3-030-72115-2 978-3-030-72116-9},
	shorttitle = {Handbook of {Artificial} {Intelligence} for {Music}},
	url = {https://link.springer.com/10.1007/978-3-030-72116-9},
	language = {en},
	urldate = {2023-05-23},
	publisher = {Springer International Publishing},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9},
	file = {Miranda - 2021 - Handbook of Artificial Intelligence for Music Fou.pdf:/Users/admin 1/Zotero/storage/2XUWXK5A/Miranda - 2021 - Handbook of Artificial Intelligence for Music Fou.pdf:application/pdf},
}

@article{shneiderman_creativity_2002,
	title = {Creativity support tools},
	volume = {45},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/570907.570945},
	doi = {10.1145/570907.570945},
	abstract = {Establishing a framework of activities for creative work.},
	number = {10},
	urldate = {2023-05-24},
	journal = {Communications of the ACM},
	author = {Shneiderman, Ben},
	month = oct,
	year = {2002},
	pages = {116--120},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/JPPW9C2Z/Shneiderman - 2002 - Creativity support tools.pdf:application/pdf},
}

@article{noauthor_democratizing_nodate,
	title = {Democratizing {Innovation}},
	file = {Democratizing Innovation.pdf:/Users/admin 1/Zotero/storage/TM5FHFCP/Democratizing Innovation.pdf:application/pdf},
}

@book{huang_sketch-based_2021,
	title = {Sketch-based {Creativity} {Support} {Tools} using {Deep} {Learning}},
	url = {http://arxiv.org/abs/2111.09991},
	abstract = {Sketching is a natural and effective visual communication medium commonly used in creative processes. Recent developments in deep-learning models drastically improved machines' ability in understanding and generating visual content. An exciting area of development explores deep-learning approaches used to model human sketches, opening opportunities for creative applications. This chapter describes three fundamental steps in developing deep-learning-driven creativity support tools that consumes and generates sketches: 1) a data collection effort that generated a new paired dataset between sketches and mobile user interfaces; 2) a sketch-based user interface retrieval system adapted from state-of-the-art computer vision techniques; and, 3) a conversational sketching system that supports the novel interaction of a natural-language-based sketch/critique authoring process. In this chapter, we survey relevant prior work in both the deep-learning and human-computer-interaction communities, document the data collection process and the systems' architectures in detail, present qualitative and quantitative results, and paint the landscape of several future research directions in this exciting area.},
	urldate = {2023-05-24},
	author = {Huang, Forrest and Schoop, Eldon and Ha, David and Nichols, Jeffrey and Canny, John},
	year = {2021},
	doi = {10.1007/978-3-030-82681-9},
	note = {arXiv:2111.09991 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/FB6SZGVT/Huang et al. - 2021 - Sketch-based Creativity Support Tools using Deep L.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/UHSHK6MI/2111.html:text/html},
}

@inproceedings{remy_evaluating_2020,
	address = {Eindhoven Netherlands},
	title = {Evaluating {Creativity} {Support} {Tools} in {HCI} {Research}},
	isbn = {978-1-4503-6974-9},
	url = {https://dl.acm.org/doi/10.1145/3357236.3395474},
	doi = {10.1145/3357236.3395474},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {Proceedings of the 2020 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {ACM},
	author = {Remy, Christian and MacDonald Vermeulen, Lindsay and Frich, Jonas and Biskjaer, Michael Mose and Dalsgaard, Peter},
	month = jul,
	year = {2020},
	pages = {457--476},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/DQMK64CG/Remy et al. - 2020 - Evaluating Creativity Support Tools in HCI Researc.pdf:application/pdf},
}

@inproceedings{schlagowski_flow_2022,
	address = {New York, NY, USA},
	series = {C\&amp;{C} '22},
	title = {Flow with the {Beat}! {Human}-{Centered} {Design} of {Virtual} {Environments} for {Musical} {Creativity} {Support} in {VR}},
	isbn = {978-1-4503-9327-0},
	url = {https://dl.acm.org/doi/10.1145/3527927.3532799},
	doi = {10.1145/3527927.3532799},
	abstract = {As previous studies have shown, the environment of creative people can have a significant impact on their creative process and thus on their creations. However, with the advent of digital tools such as virtual instruments and digital audio workstations, more and more creative work is digital and decoupled from the creator’s environment. Virtual Reality technologies open up new possibilities here, as creative tools can seamlessly merge with any virtual environment the user finds himself in. This paper reports on the human-centered design process of a VR application that aims at supporting the user’s individual needs to support their creativity while composing percussive beats in virtual environments. For this purpose, we derived factors that influence creativity from literature and conducted focus group interviews in order to learn how virtual environments and 3DUI can be designed for creativity support. In a subsequent laboratory study, we let users interact with a virtual step sequencer UI in virtual environments that were either customizable or fixed/unchangeable. By analyzing post-test ratings from music experts, self-report questionnaires, and user behavior data, we examined the effects of such customizable virtual environments on user creativity, user experience, flow, and subjective creativity support scales. While we did not observe a significant impact of this independent variable on user creativity, user experience or flow, we found that users had specific individual needs regarding their virtual surroundings and strongly preferred customizable virtual environments, even though the fixed virtual environment was designed to be creatively stimulating. We also observed consistently high flow and user experience ratings, which promote human-centered design of VR-based creativity support tools in a musical context.},
	urldate = {2023-05-24},
	booktitle = {Creativity and {Cognition}},
	publisher = {Association for Computing Machinery},
	author = {Schlagowski, Ruben and Wildgrube, Fabian and Mertes, Silvan and George, Ceenu and André, Elisabeth},
	month = jun,
	year = {2022},
	keywords = {Creativity, Creativity Support Tools, Generative Adversarial Networks, Musical XR, User Centered Design, User Interface Design, Virtual Reality},
	pages = {428--442},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/IHK3QIHX/Schlagowski et al. - 2022 - Flow with the Beat! Human-Centered Design of Virtu.pdf:application/pdf},
}

@inproceedings{nash_liveness_2012,
	title = {Liveness and {Flow} in {Notation} {Use}},
	abstract = {This paper presents concepts, models, and empirical findings relating to liveness and flow in the user experience of systems mediated by notation. Results from an extensive two-year field study of over 1,000 sequencer and tracker users, combining interaction logging, user surveys, and a video study, are used to illustrate the properties of notations and interfaces that facilitate greater immersion in musical activities and domains, borrowing concepts from programming to illustrate the role of visual and musical feedback, from the notation and domain respectively. The Cognitive Dimensions of Notations framework and Csikszentmihalyi’s flow theory are combined to demonstrate how non-realtime, notation-mediated interaction can support focused, immersive, energetic, and intrinsically rewarding musical experiences, and to what extent they are supported in the interfaces of music production software. Users are shown to maintain liveness through a rapid, iterative editaudition cycle that integrates audio and visual feedback.},
	author = {Nash, Chris and Blackwell, Alan},
	month = may,
	year = {2012},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/8A43K4AB/Nash and Blackwell - 2012 - Liveness and Flow in Notation Use.pdf:application/pdf},
}

@misc{bloomsburycom_music_nodate,
	title = {Music as {Multimodal} {Discourse}},
	url = {https://www.bloomsbury.com/uk/music-as-multimodal-discourse-9781474264426/},
	abstract = {We communicate multimodally. Everyday communication involves not only words, but gestures, images, videos, sounds and of course, music. Music has traditionally…},
	language = {en},
	urldate = {2023-06-20},
	journal = {Bloomsbury},
	author = {bloomsbury.com},
}

@article{cross_music_nodate,
	title = {Music, {Cognition}, {Culture}, and {Evolution}},
	abstract = {We seem able to define the biological foundations for our musicality within a clear and unitary framework, yet music itself does not appear so clearly definable. Music is different things and does different things in different cultures; the bundles of elements and functions that are music for any given culture may overlap minimally with those of another culture, even for those cultures where “music” constitutes a discrete and identifiable category of human activity in its own right. The dynamics of culture, of music as cultural praxis, are neither necessarily reducible, nor easily relatable, to the dynamics of our biologies. Yet music appears to be a universal human competence. Recent evolutionary theory, however, affords a means for exploring things biological and cultural within a framework in which they are at least commensurable. The adoption of this perspective shifts the focus of the search for the foundations of music away from the mature and particular expression of music within a specific culture or situation and on to the human capacity for musicality. This paper will survey recent research that examines that capacity and its evolutionary origins in the light of a definition of music that embraces music’s multifariousness. It will be suggested that music, like speech, is a product of both our biologies and our social interactions; that music is a necessary and integral dimension of human development; and that music may have played a central role in the evolution of the modern human mind.},
	language = {en},
	author = {Cross, Ian},
	file = {Cross - Music, Cognition, Culture, and Evolution.pdf:/Users/admin 1/Zotero/storage/VBNTKJ6K/Cross - Music, Cognition, Culture, and Evolution.pdf:application/pdf},
}

@article{granshaw_neural_2021,
	title = {Neural networks and neurodiversity},
	volume = {36},
	copyright = {© 2021 The Author. The Photogrammetric Record © 2021 The Remote Sensing and Photogrammetry Society and John Wiley \& Sons Ltd},
	issn = {1477-9730},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phor.12376},
	doi = {10.1111/phor.12376},
	language = {en},
	number = {175},
	urldate = {2023-06-20},
	journal = {The Photogrammetric Record},
	author = {Granshaw, Stuart I.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phor.12376},
	pages = {192--196},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/R25GBTU6/Granshaw - 2021 - Neural networks and neurodiversity.pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/39C9SEYD/phor.html:text/html},
}

@book{wiggins_computational_2011,
	title = {Computational {Modeling} of {Music} {Cognition} and {Musical} {Creativity}},
	url = {https://academic.oup.com/edited-volume/28207/chapter/213193131},
	language = {en},
	urldate = {2023-06-20},
	publisher = {Oxford University Press},
	author = {Wiggins, Geraint A. and Pearce, Marcus T. and Müllensiefen, Daniel},
	month = apr,
	year = {2011},
	doi = {10.1093/oxfordhb/9780199792030.013.0019},
	file = {Wiggins et al. - 2011 - Computational Modeling of Music Cognition and Musi.pdf:/Users/admin 1/Zotero/storage/MJSHPEZL/Wiggins et al. - 2011 - Computational Modeling of Music Cognition and Musi.pdf:application/pdf},
}

@article{brown_techniques_2015,
	title = {Techniques for {Generative} {Melodies} {Inspired} by {Music} {Cognition}},
	volume = {39},
	issn = {0148-9267},
	doi = {10.1162/COMJ_a_00282},
	abstract = {This article presents a series of algorithmic techniques for melody generation, inspired by models of music cognition. The techniques are designed for interactive composition, and so privilege brevity, simplicity, and flexibility over fidelity to the underlying models. The cognitive models canvassed span gestalt, preference rule, and statistical learning perspectives; this is a diverse collection with a common thread—the centrality of “expectations” to music cognition. We operationalize some recurrent themes across this collection as probabilistic descriptions of melodic tendency, codifying them as stochastic melody-generation techniques. The techniques are combined into a concise melody generator, with salient parameters exposed for ready manipulation in real time. These techniques may be especially relevant to algorithmic composers, the live-coding community, and to music psychologists and theorists interested in how computational interpretations of cognitive models “sound” in practice.},
	number = {1},
	journal = {Computer Music Journal},
	author = {Brown, Andrew R. and Gifford, Toby and Davidson, Robert},
	month = mar,
	year = {2015},
	note = {Conference Name: Computer Music Journal},
	pages = {11--26},
	file = {Full Text:/Users/admin 1/Zotero/storage/SPLLCX5I/Brown et al. - 2015 - Techniques for Generative Melodies Inspired by Mus.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/admin 1/Zotero/storage/99KUFC88/7067562.html:text/html},
}

@article{collins_remixing_nodate,
	title = {Remixing {AIs}: mind swaps, hybrainity, and splicing musical models},
	abstract = {Musical models created using machine learning techniques can be hacked, repurposed and spliced in many new ways. The current generation of models is not at any level of simulated consciousness sufficient to trigger immediate ethical blocks, and stands open to systems surgery. Beyond morphing of inputs and outputs, hidden layers of deep learning models may themselves be actively stimulated and substituted, from unit weights and biases to activation functions. Layers from multiple networks may be swapped and interpolated, from single units to complete layers. The hybridity of musical formation takes place at the level of model internals, in artistic transformations beyond standard transfer learning. This activity of AI code bending is dubbed here ‘hybrainity’, and alongside theoretical discussion of its origins, potential and ethics, examples of hacking particular machine learning models for new creative projects are provided, including applications in live performance and audiovisual generation.},
	language = {en},
	author = {Collins, Nick},
	file = {Collins - Remixing AIs mind swaps, hybrainity, and splicing.pdf:/Users/admin 1/Zotero/storage/9B7XFBZG/Collins - Remixing AIs mind swaps, hybrainity, and splicing.pdf:application/pdf},
}

@misc{caillon_rave_2021,
	title = {{RAVE}: {A} variational autoencoder for fast and high-quality neural audio synthesis},
	shorttitle = {{RAVE}},
	url = {http://arxiv.org/abs/2111.05011},
	doi = {10.48550/arXiv.2111.05011},
	abstract = {Deep generative models applied to audio have improved by a large margin the state-of-the-art in many speech and music related tasks. However, as raw waveform modelling remains an inherently difficult task, audio generative models are either computationally intensive, rely on low sampling rates, are complicated to control or restrict the nature of possible signals. Among those models, Variational AutoEncoders (VAE) give control over the generation by exposing latent variables, although they usually suffer from low synthesis quality. In this paper, we introduce a Realtime Audio Variational autoEncoder (RAVE) allowing both fast and high-quality audio waveform synthesis. We introduce a novel two-stage training procedure, namely representation learning and adversarial fine-tuning. We show that using a post-training analysis of the latent space allows a direct control between the reconstruction fidelity and the representation compactness. By leveraging a multi-band decomposition of the raw waveform, we show that our model is the first able to generate 48kHz audio signals, while simultaneously running 20 times faster than real-time on a standard laptop CPU. We evaluate synthesis quality using both quantitative and qualitative subjective experiments and show the superiority of our approach compared to existing models. Finally, we present applications of our model for timbre transfer and signal compression. All of our source code and audio examples are publicly available.},
	urldate = {2023-06-28},
	publisher = {arXiv},
	author = {Caillon, Antoine and Esling, Philippe},
	month = dec,
	year = {2021},
	note = {arXiv:2111.05011 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/9NFCENBI/Caillon and Esling - 2021 - RAVE A variational autoencoder for fast and high-.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/BI6CB275/2111.html:text/html},
}

@misc{vigliensoni_small-data_2022,
	address = {Online},
	title = {A small-data mindset for generative {AI} creative work},
	url = {https://zenodo.org/record/7086327},
	abstract = {In this paper, we argue that working with small-scale datasets is an often-overlooked but powerful mechanism for enabling greater human influence over generative AI (GenAI) systems in creative contexts. We describe some of the benefits of working with small-scale data, and we argue that conventional ways of thinking about the value of large data, such as preventing overfitting, are not always well-matched to creative aims. We discuss how models built with small-scale data can facilitate meaningful creative work, providing examples from text, image, and sound.},
	language = {eng},
	urldate = {2023-06-28},
	author = {Vigliensoni, Gabriel and Perry, Phoenix and Fiebrink, Rebecca},
	month = may,
	year = {2022},
	doi = {10.5281/zenodo.7086327},
	note = {Publisher: Zenodo},
	keywords = {creative AI, generative systems, small-data},
	file = {Zenodo Full Text PDF:/Users/admin 1/Zotero/storage/VYGL85YB/Vigliensoni et al. - 2022 - A small-data mindset for generative AI creative wo.pdf:application/pdf},
}

@inproceedings{scurto_soundwalking_2023,
	title = {Soundwalking {Deep} {Latent} {Spaces}},
	url = {https://hal.science/hal-04108997},
	abstract = {This paper relates an early art-research collaboration between two practitioners in machine learning and virtual worlds toward new embodied musical experiences of Artificial Intelligence (AI). Instead of a digital music instrument or a music-generating agent, we propose to craft a soundwalk experience where a human person moves through a three-dimensional virtual world to explore a latent sound space generated by deep learning. We report on the diffractive prototyping and iterative crafting of three such soundwalks through/out deep latent spaces, using nn∼ and New Atlantis as computational platforms for AI audio processing and virtual world experimentation. We share critical perspectives emerging from our latent soundwalking practice, with the hope that they contribute to ongoing communitywide reflections toward new AI for musical expression.},
	language = {en},
	urldate = {2023-06-28},
	author = {Scurto, Hugo and Postel, Ludmila},
	month = may,
	year = {2023},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/FCCT4YUT/Scurto and Postel - 2023 - Soundwalking Deep Latent Spaces.pdf:application/pdf},
}

@misc{hadjeres_glsr-vae_2017,
	title = {{GLSR}-{VAE}: {Geodesic} {Latent} {Space} {Regularization} for {Variational} {AutoEncoder} {Architectures}},
	shorttitle = {{GLSR}-{VAE}},
	url = {http://arxiv.org/abs/1707.04588},
	abstract = {VAEs (Variational AutoEncoders) have proved to be powerful in the context of density modeling and have been used in a variety of contexts for creative purposes. In many settings, the data we model possesses continuous attributes that we would like to take into account at generation time. We propose in this paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational AutoEncoder architecture and its generalizations which allows a fine control on the embedding of the data into the latent space. When augmenting the VAE loss with this regularization, changes in the learned latent space reflects changes of the attributes of the data. This deeper understanding of the VAE latent space structure offers the possibility to modulate the attributes of the generated data in a continuous way. We demonstrate its efficiency on a monophonic music generation task where we manage to generate variations of discrete sequences in an intended and playful way.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Hadjeres, Gaëtan and Nielsen, Frank and Pachet, François},
	month = jul,
	year = {2017},
	note = {arXiv:1707.04588 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/admin 1/Zotero/storage/KD6RVD82/1707.html:text/html;Full Text PDF:/Users/admin 1/Zotero/storage/WY86C95N/Hadjeres et al. - 2017 - GLSR-VAE Geodesic Latent Space Regularization for.pdf:application/pdf},
}

@book{pati_latent_2019,
	title = {Latent {Space} {Regularization} for {Explicit} {Control} of {Musical} {Attributes}},
	abstract = {Deep generative models for music are often restrictive since they do not allow users any meaningful control over the generated music. To address this issue, we propose a novel latent space regularization technique which is capable of structuring the latent space of a deep generative model by encoding musically meaningful attributes along specific dimensions of the latent space. This, in turn, can provide users with explicit control over these attributes during inference and thereby, help design intuitive musical interfaces to enhance creative workflows.},
	author = {Pati, Ashis and Lerch, Alexander},
	month = jun,
	year = {2019},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/BZ8BTV6H/Pati and Lerch - 2019 - Latent Space Regularization for Explicit Control o.pdf:application/pdf},
}

@inproceedings{barbato_latent_2021,
	address = {Nashville, TN, USA},
	title = {Latent {Space} {Regularization} for {Unsupervised} {Domain} {Adaptation} in {Semantic} {Segmentation}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9523097/},
	doi = {10.1109/CVPRW53098.2021.00318},
	abstract = {Deep convolutional neural networks for semantic segmentation achieve outstanding accuracy, however they also have a couple of major drawbacks: ﬁrst, they do not generalize well to distributions slightly different from the one of the training data; second, they require a huge amount of labeled data for their optimization. In this paper, we introduce feature-level space-shaping regularization strategies to reduce the domain discrepancy in semantic segmentation. In particular, for this purpose we jointly enforce a clustering objective, a perpendicularity constraint and a norm alignment goal on the feature vectors corresponding to source and target samples. Additionally, we propose a novel measure able to capture the relative efﬁcacy of an adaptation strategy compared to supervised training. We verify the effectiveness of such methods in the autonomous driving setting achieving state-of-the-art results in multiple syntheticto-real road scenes benchmarks.},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Barbato, Francesco and Toldo, Marco and Michieli, Umberto and Zanuttigh, Pietro},
	month = jun,
	year = {2021},
	pages = {2829--2839},
	file = {Barbato et al. - 2021 - Latent Space Regularization for Unsupervised Domai.pdf:/Users/admin 1/Zotero/storage/94DGIT8U/Barbato et al. - 2021 - Latent Space Regularization for Unsupervised Domai.pdf:application/pdf},
}

@misc{toldo_unsupervised_2020,
	title = {Unsupervised {Domain} {Adaptation} in {Semantic} {Segmentation} via {Orthogonal} and {Clustered} {Embeddings}},
	url = {http://arxiv.org/abs/2011.12616},
	doi = {10.48550/arXiv.2011.12616},
	abstract = {Deep learning frameworks allowed for a remarkable advancement in semantic segmentation, but the data hungry nature of convolutional networks has rapidly raised the demand for adaptation techniques able to transfer learned knowledge from label-abundant domains to unlabeled ones. In this paper we propose an effective Unsupervised Domain Adaptation (UDA) strategy, based on a feature clustering method that captures the different semantic modes of the feature distribution and groups features of the same class into tight and well-separated clusters. Furthermore, we introduce two novel learning objectives to enhance the discriminative clustering performance: an orthogonality loss forces spaced out individual representations to be orthogonal, while a sparsity loss reduces class-wise the number of active feature channels. The joint effect of these modules is to regularize the structure of the feature space. Extensive evaluations in the synthetic-to-real scenario show that we achieve state-of-the-art performance.},
	urldate = {2023-06-30},
	publisher = {arXiv},
	author = {Toldo, Marco and Michieli, Umberto and Zanuttigh, Pietro},
	month = nov,
	year = {2020},
	note = {arXiv:2011.12616 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/LWQSKIKF/Toldo et al. - 2020 - Unsupervised Domain Adaptation in Semantic Segment.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/H69SSEC9/2011.html:text/html},
}

@article{roche_make_2021,
	title = {Make {That} {Sound} {More} {Metallic}: {Towards} a {Perceptually} {Relevant} {Control} of the {Timbre} of {Synthesizer} {Sounds} {Using} a {Variational} {Autoencoder}},
	volume = {4},
	shorttitle = {Make {That} {Sound} {More} {Metallic}},
	url = {https://hal.science/hal-03247371},
	doi = {10.5334/tismir.76},
	abstract = {In this article, we propose a new method of sound transformation based on control parameters that are intuitive and relevant for musicians. This method uses a variational autoencoder (VAE) model that is first trained in an unsupervised manner on a large dataset of synthesizer sounds. Then, a perceptual regularization term is added to the loss function to be optimized, and a supervised fine-tuning of the model is carried out using a small subset of perceptually labeled sounds. The labels were obtained from a perceptual test of Verbal Attribute Magnitude Estimation in which listeners rated this training sound dataset along eight perceptual dimensions (French equivalents of metallic, warm, breathy, vibrating, percussive, resonating, evolving, aggressive). These dimensions were identified as relevant for the description of synthesizer sounds in a first Free Verbalization test. The resulting VAE model was evaluated by objective reconstruction measures and a perceptual test. Both showed that the model was able, to a certain extent, to capture the acoustic properties of most of the perceptual dimensions and to transform sound timbre along at least two of them (aggressive and vibrating) in a perceptually relevant manner. Moreover, it was able to generalize to unseen samples even though a small set of labeled sounds was used.},
	language = {en},
	urldate = {2023-06-30},
	journal = {Transactions of the International Society for Music Information Retrieval (TISMIR)},
	author = {Roche, Fanny and Hueber, Thomas and Garnier, Maëva and Limier, Samuel and Girin, Laurent},
	month = may,
	year = {2021},
	pages = {52},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/Q8W46GBK/Roche et al. - 2021 - Make That Sound More Metallic Towards a Perceptua.pdf:application/pdf},
}

@article{giordano_what_2022,
	title = {What do we mean with sound semantics, exactly? {A} survey of taxonomies and ontologies of everyday sounds},
	volume = {13},
	issn = {1664-1078},
	shorttitle = {What do we mean with sound semantics, exactly?},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2022.964209},
	abstract = {Taxonomies and ontologies for the characterization of everyday sounds have been developed in several research fields, including auditory cognition, soundscape research, artificial hearing, sound design, and medicine. Here, we surveyed 36 of such knowledge organization systems, which we identified through a systematic literature search. To evaluate the semantic domains covered by these systems within a homogeneous framework, we introduced a comprehensive set of verbal sound descriptors (sound source properties; attributes of sensation; sound signal descriptors; onomatopoeias; music genres), which we used to manually label the surveyed descriptor classes. We reveal that most taxonomies and ontologies were developed to characterize higher-level semantic relations between sound sources in terms of the sound-generating objects and actions involved (what/how), or in terms of the environmental context (where). This indicates the current lack of a comprehensive ontology of everyday sounds that covers simultaneously all semantic aspects of the relation between sounds. Such an ontology may have a wide range of applications and purposes, ranging from extending our scientific knowledge of auditory processes in the real world, to developing artificial hearing systems.},
	urldate = {2023-06-30},
	journal = {Frontiers in Psychology},
	author = {Giordano, Bruno L. and de Miranda Azevedo, Ricardo and Plasencia-Calaña, Yenisel and Formisano, Elia and Dumontier, Michel},
	year = {2022},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/W75FU56E/Giordano et al. - 2022 - What do we mean with sound semantics, exactly A s.pdf:application/pdf},
}

@article{hiller_musical_1958,
	title = {Musical {Composition} with a {High}-{Speed} {Digital} {Computer}},
	volume = {6},
	url = {https://www.aes.org/e-lib/browse.cfm?elib=231},
	abstract = {A technique has been developed for writing music by means of automatic high speed digital computers such as ILLIAC, located at the Univeristy of Illinois. Random integers, considered the equivalent of musical notes, are first generated and then screened by mathematical operations which express the rules of composition. The control over the musical output is limited solely by the input instructions, and factors not specifically accounted for are left entirely to chance. Studies of the problems...},
	language = {English},
	number = {3},
	urldate = {2023-07-01},
	journal = {Journal of the Audio Engineering Society},
	author = {Hiller, Jr and Isaacson, L. M.},
	month = jul,
	year = {1958},
	note = {Publisher: Audio Engineering Society},
	pages = {154--160},
}

@book{puckette_pure_1996,
	title = {Pure {Data}},
	author = {Puckette, Miller},
	month = jan,
	year = {1996},
	note = {Pages: 41},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/6YLYWR85/Puckette - 1996 - Pure Data.pdf:application/pdf},
}

@article{mccartney_rethinking_2002,
	title = {Rethinking the {Computer} {Music} {Language}: {Super} {Collider}},
	volume = {26},
	issn = {0148-9267},
	shorttitle = {Rethinking the {Computer} {Music} {Language}},
	url = {https://www.jstor.org/stable/3681770},
	number = {4},
	urldate = {2023-07-01},
	journal = {Computer Music Journal},
	author = {McCartney, James},
	year = {2002},
	note = {Publisher: The MIT Press},
	pages = {61--68},
	file = {JSTOR Full Text PDF:/Users/admin 1/Zotero/storage/229A2QRY/McCartney - 2002 - Rethinking the Computer Music Language Super Coll.pdf:application/pdf},
}

@misc{waite_melodyrnn_2016,
	title = {{MelodyRNN} in {Project} {Magenta}: {Generating} long-term structure in songs and stories},
	url = {https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn},
	urldate = {2023-05-01},
	author = {Waite, Elliot and Eck, Douglas and {Adam Roberts} and Abolafia, Dan},
	year = {2016},
}

@inproceedings{johnson_generating_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Generating {Polyphonic} {Music} {Using} {Tied} {Parallel} {Networks}},
	isbn = {978-3-319-55750-2},
	doi = {10.1007/978-3-319-55750-2_9},
	abstract = {We describe a neural network architecture which enables prediction and composition of polyphonic music in a manner that preserves translation-invariance of the dataset. Specifically, we demonstrate training a probabilistic model of polyphonic music using a set of parallel, tied-weight recurrent networks, inspired by the structure of convolutional neural networks. This model is designed to be invariant to transpositions, but otherwise is intentionally given minimal information about the musical domain, and tasked with discovering patterns present in the source dataset. We present two versions of the model, denoted TP-LSTM-NADE and BALSTM, and also give methods for training the network and for generating novel music. This approach attains high performance at a musical prediction task and successfully creates note sequences which possess measure-level musical structure.},
	language = {en},
	booktitle = {Computational {Intelligence} in {Music}, {Sound}, {Art} and {Design}},
	publisher = {Springer International Publishing},
	author = {Johnson, Daniel D.},
	editor = {Correia, João and Ciesielski, Vic and Liapis, Antonios},
	year = {2017},
	keywords = {Convolutional Neural Network, Joint Probability Distribution, Prediction Task, Recurrent Neural Network, Translation Invariance},
	pages = {128--143},
}

@misc{noauthor_musicvae_nodate,
	title = {{MusicVAE}: {Creating} a palette for musical scores with machine learning.},
	url = {https://magenta.tensorflow.org/music-vae},
	urldate = {2023-07-02},
	file = {MusicVAE\: Creating a palette for musical scores with machine learning.:/Users/admin 1/Zotero/storage/Z6F9EVMS/music-vae.html:text/html},
}

@article{oord_wavenet_2016,
	title = {Wavenet: {A} generative model for raw audio},
	shorttitle = {Wavenet},
	journal = {arXiv preprint arXiv:1609.03499},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	year = {2016},
}

@misc{caillon_rave_2021-1,
	title = {{RAVE}: {A} variational autoencoder for fast and high-quality neural audio synthesis},
	shorttitle = {{RAVE}},
	url = {http://arxiv.org/abs/2111.05011},
	doi = {10.48550/arXiv.2111.05011},
	abstract = {Deep generative models applied to audio have improved by a large margin the state-of-the-art in many speech and music related tasks. However, as raw waveform modelling remains an inherently difficult task, audio generative models are either computationally intensive, rely on low sampling rates, are complicated to control or restrict the nature of possible signals. Among those models, Variational AutoEncoders (VAE) give control over the generation by exposing latent variables, although they usually suffer from low synthesis quality. In this paper, we introduce a Realtime Audio Variational autoEncoder (RAVE) allowing both fast and high-quality audio waveform synthesis. We introduce a novel two-stage training procedure, namely representation learning and adversarial fine-tuning. We show that using a post-training analysis of the latent space allows a direct control between the reconstruction fidelity and the representation compactness. By leveraging a multi-band decomposition of the raw waveform, we show that our model is the first able to generate 48kHz audio signals, while simultaneously running 20 times faster than real-time on a standard laptop CPU. We evaluate synthesis quality using both quantitative and qualitative subjective experiments and show the superiority of our approach compared to existing models. Finally, we present applications of our model for timbre transfer and signal compression. All of our source code and audio examples are publicly available.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Caillon, Antoine and Esling, Philippe},
	month = dec,
	year = {2021},
	note = {arXiv:2111.05011 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/TUJUALHL/Caillon and Esling - 2021 - RAVE A variational autoencoder for fast and high-.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/QKURJEUI/2111.html:text/html},
}

@article{fyfe_neutone_2022,
	title = {Neutone - {Real}-time {AI} audio plugin for {DAWs}},
	url = {https://qmro.qmul.ac.uk/xmlui/handle/123456789/88475},
	abstract = {When music and audio creators want to experiment with the latest AI technologies such as deep learning, there is currently a steep learning curve for programming in Python with libraries such as PyTorch and using dedicated processing hardware such as GPU. Neutone solves these existing challenges by developing a VST/AU plugin that can be run in real-time on CPU in DAWs. A number of timbre transfer models and deep learning powered effects have already been published for this plugin and can be downloaded at runtime.},
	language = {en},
	urldate = {2023-07-02},
	author = {Fyfe, A. and Mitcheltree, C.},
	month = nov,
	year = {2022},
	note = {Accepted: 2023-05-26T15:33:37Z},
	file = {Snapshot:/Users/admin 1/Zotero/storage/5TL9S3VU/88475.html:text/html},
}

@misc{caillon_streamable_2022,
	title = {Streamable {Neural} {Audio} {Synthesis} {With} {Non}-{Causal} {Convolutions}},
	url = {http://arxiv.org/abs/2204.07064},
	doi = {10.48550/arXiv.2204.07064},
	abstract = {Deep learning models are mostly used in an offline inference fashion. However, this strongly limits the use of these models inside audio generation setups, as most creative workflows are based on real-time digital signal processing. Although approaches based on recurrent networks can be naturally adapted to this buffer-based computation, the use of convolutions still poses some serious challenges. To tackle this issue, the use of causal streaming convolutions have been proposed. However, this requires specific complexified training and can impact the resulting audio quality. In this paper, we introduce a new method allowing to produce non-causal streaming models. This allows to make any convolutional model compatible with real-time buffer-based processing. As our method is based on a post-training reconfiguration of the model, we show that it is able to transform models trained without causal constraints into a streaming model. We show how our method can be adapted to fit complex architectures with parallel branches. To evaluate our method, we apply it on the recent RAVE model, which provides high-quality real-time audio synthesis. We test our approach on multiple music and speech datasets and show that it is faster than overlap-add methods, while having no impact on the generation quality. Finally, we introduce two open-source implementation of our work as Max/MSP and PureData externals, and as a VST audio plugin. This allows to endow traditional digital audio workstation with real-time neural audio synthesis on a laptop CPU.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Caillon, Antoine and Esling, Philippe},
	month = apr,
	year = {2022},
	note = {arXiv:2204.07064 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/93VJPWTL/Caillon and Esling - 2022 - Streamable Neural Audio Synthesis With Non-Causal .pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/V5LS99LE/2204.html:text/html},
}

@misc{hayes_neural_2021,
	title = {Neural {Waveshaping} {Synthesis}},
	url = {http://arxiv.org/abs/2107.05050},
	doi = {10.48550/arXiv.2107.05050},
	abstract = {We present the Neural Waveshaping Unit (NEWT): a novel, lightweight, fully causal approach to neural audio synthesis which operates directly in the waveform domain, with an accompanying optimisation (FastNEWT) for efficient CPU inference. The NEWT uses time-distributed multilayer perceptrons with periodic activations to implicitly learn nonlinear transfer functions that encode the characteristics of a target timbre. Once trained, a NEWT can produce complex timbral evolutions by simple affine transformations of its input and output signals. We paired the NEWT with a differentiable noise synthesiser and reverb and found it capable of generating realistic musical instrument performances with only 260k total model parameters, conditioned on F0 and loudness features. We compared our method to state-of-the-art benchmarks with a multi-stimulus listening test and the Fr{\textbackslash}'echet Audio Distance and found it performed competitively across the tested timbral domains. Our method significantly outperformed the benchmarks in terms of generation speed, and achieved real-time performance on a consumer CPU, both with and without FastNEWT, suggesting it is a viable basis for future creative sound design tools.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Hayes, Ben and Saitis, Charalampos and Fazekas, György},
	month = jul,
	year = {2021},
	note = {arXiv:2107.05050 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/KMMYHN9L/Hayes et al. - 2021 - Neural Waveshaping Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/YRIL4IB5/2107.html:text/html},
}

@misc{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	doi = {10.48550/arXiv.1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv:1704.01279 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/XKA9UN3R/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/4IBCMZI5/1704.html:text/html},
}

@inproceedings{scholz_nonlinear_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computational} {Science} and {Enginee}},
	title = {Nonlinear {Principal} {Component} {Analysis}: {Neural} {Network} {Models} and {Applications}},
	isbn = {978-3-540-73750-6},
	shorttitle = {Nonlinear {Principal} {Component} {Analysis}},
	doi = {10.1007/978-3-540-73750-6_2},
	abstract = {Nonlinear principal component analysis (NLPCA) as a nonlinear generalisation of standard principal component analysis (PCA) means to generalise the principal components from straight lines to curves. This chapter aims to provide an extensive description of the autoassociative neural network approach for NLPCA. Several network architectures will be discussed including the hierarchical, the circular, and the inverse model with special emphasis to missing data. Results are shown from applications in the field of molecular biology. This includes metabolite data analysis of a cold stress experiment in the model plant Arabidopsis thaliana and gene expression analysis of the reproductive cycle of the malaria parasite Plasmodium falciparum within infected red blood cells.},
	language = {en},
	booktitle = {Principal {Manifolds} for {Data} {Visualization} and {Dimension} {Reduction}},
	publisher = {Springer},
	author = {Scholz, Matthias and Fraunholz, Martin and Selbig, Joachim},
	editor = {Gorban, Alexander N. and Kégl, Balázs and Wunsch, Donald C. and Zinovyev, Andrei Y.},
	year = {2008},
	keywords = {Independent Component Analysis, Kernel Principal Component Analysis, Neural Network Model, Nonlinear Component, Nonlinear Dimensionality Reduction},
	pages = {44--67},
	file = {Submitted Version:/Users/admin 1/Zotero/storage/57Y4TJQ4/Scholz et al. - 2008 - Nonlinear Principal Component Analysis Neural Net.pdf:application/pdf},
}

@article{richardson_principal_nodate,
	title = {Principal {Component} {Analysis}},
	language = {en},
	author = {Richardson, Mark},
	file = {Richardson - Principal Component Analysis.pdf:/Users/admin 1/Zotero/storage/ZX7G4NH2/Richardson - Principal Component Analysis.pdf:application/pdf},
}

@techreport{smith_tutorial_2002,
	title = {A tutorial on principal components analysis},
	institution = {Department of Computer Science, University of Otago},
	author = {Smith, Lindsay I.},
	year = {2002},
	file = {Full Text:/Users/admin 1/Zotero/storage/FQXQT4CD/Smith - 2002 - A tutorial on principal components analysis.pdf:application/pdf},
}

@article{smith_tutorial_nodate,
	title = {A tutorial on {Principal} {Components} {Analysis}},
	language = {en},
	author = {Smith, Lindsay I},
	file = {Smith - A tutorial on Principal Components Analysis.pdf:/Users/admin 1/Zotero/storage/SK3GL69V/Smith - A tutorial on Principal Components Analysis.pdf:application/pdf},
}

@article{hasan_review_2021,
	title = {A {Review} of {Principal} {Component} {Analysis} {Algorithm} for {Dimensionality} {Reduction}},
	volume = {2},
	copyright = {Copyright (c) 2021 Journal of Soft Computing and Data Mining},
	issn = {2716-621X},
	url = {https://publisher.uthm.edu.my/ojs/index.php/jscdm/article/view/8032},
	abstract = {Big databases are increasingly widespread and are therefore hard to understand, in exploratory biomedicine science, big data in health research is highly exciting because data-based analyses can travel quicker than hypothesis-based research. Principal Component Analysis (PCA) is a method to reduce the dimensionality of certain datasets. Improves interpretability but without losing much information. It achieves this by creating new covariates that are not related to each other. Finding those new variables, or what we call the main components, will reduce the eigenvalue /eigenvectors solution problem. (PCA) can be said to be an adaptive data analysis technology because technology variables are developed to adapt to different data types and structures. This review will start by introducing the basic ideas of (PCA), describe some concepts related to (PCA), and discussing. What it can do, and reviewed fifteen articles of (PCA) that have been introduced and published in the last three years.},
	language = {en},
	number = {1},
	urldate = {2023-07-02},
	journal = {Journal of Soft Computing and Data Mining},
	author = {Hasan, Basna Mohammed Salih and Abdulazeez, Adnan Mohsin},
	month = apr,
	year = {2021},
	note = {Number: 1},
	keywords = {dimensionality reduction},
	pages = {20--30},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/IDL55EU7/Hasan and Abdulazeez - 2021 - A Review of Principal Component Analysis Algorithm.pdf:application/pdf},
}

@misc{tschannen_recent_2018,
	title = {Recent {Advances} in {Autoencoder}-{Based} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1812.05069},
	doi = {10.48550/arXiv.1812.05069},
	abstract = {Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
	month = dec,
	year = {2018},
	note = {arXiv:1812.05069 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/9VPSZIXM/Tschannen et al. - 2018 - Recent Advances in Autoencoder-Based Representatio.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/B7G8QF2U/1812.html:text/html},
}

@inproceedings{zhai_autoencoder_2018,
	title = {Autoencoder and {Its} {Various} {Variants}},
	doi = {10.1109/SMC.2018.00080},
	abstract = {The concept of autoencoder was originally proposed by LeCun in 1987, early works on autoencoder were used for dimensionality reduction or feature learning. Recently, with the popularity of deep learning research, autoencoder has been brought to the forefront of generative modeling. Many variants of autoencoder have been proposed by different researchers and have been successfully applied in many fields, such as computer vision, speech recognition and natural language processing. In this paper, we present a comprehensive survey on autoencoder and its various variants. Furthermore, we also present the lineage of the surveyed autoencoders. This paper can provide researchers engaged in related works with very valuable help.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Zhai, Junhai and Zhang, Sufang and Chen, Junfen and He, Qiang},
	month = oct,
	year = {2018},
	note = {ISSN: 2577-1655},
	keywords = {autoencoder, Computational modeling, Data models, decoder, Decoding, deep learning, feature learning, Gallium nitride, Generative adversarial networks, generative model, Mathematical model, Training},
	pages = {415--419},
	file = {IEEE Xplore Full Text PDF:/Users/admin 1/Zotero/storage/M4XQIL49/Zhai et al. - 2018 - Autoencoder and Its Various Variants.pdf:application/pdf},
}

@misc{bank_autoencoders_2021,
	title = {Autoencoders},
	url = {http://arxiv.org/abs/2003.05991},
	doi = {10.48550/arXiv.2003.05991},
	abstract = {An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	month = apr,
	year = {2021},
	note = {arXiv:2003.05991 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/DBE4Q9WW/Bank et al. - 2021 - Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/LTM3J6QC/2003.html:text/html},
}

@inproceedings{zhai_autoencoder_2018-1,
	title = {Autoencoder and {Its} {Various} {Variants}},
	doi = {10.1109/SMC.2018.00080},
	abstract = {The concept of autoencoder was originally proposed by LeCun in 1987, early works on autoencoder were used for dimensionality reduction or feature learning. Recently, with the popularity of deep learning research, autoencoder has been brought to the forefront of generative modeling. Many variants of autoencoder have been proposed by different researchers and have been successfully applied in many fields, such as computer vision, speech recognition and natural language processing. In this paper, we present a comprehensive survey on autoencoder and its various variants. Furthermore, we also present the lineage of the surveyed autoencoders. This paper can provide researchers engaged in related works with very valuable help.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Zhai, Junhai and Zhang, Sufang and Chen, Junfen and He, Qiang},
	month = oct,
	year = {2018},
	note = {ISSN: 2577-1655},
	keywords = {autoencoder, Computational modeling, Data models, decoder, Decoding, deep learning, feature learning, Gallium nitride, Generative adversarial networks, generative model, Mathematical model, Training},
	pages = {415--419},
	file = {IEEE Xplore Abstract Record:/Users/admin 1/Zotero/storage/CIAEVCAH/8616075.html:text/html;IEEE Xplore Full Text PDF:/Users/admin 1/Zotero/storage/4GM654U2/Zhai et al. - 2018 - Autoencoder and Its Various Variants.pdf:application/pdf},
}

@misc{hy_multiresolution_2022,
	title = {Multiresolution {Equivariant} {Graph} {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/2106.00967},
	doi = {10.48550/arXiv.2106.00967},
	abstract = {In this paper, we propose Multiresolution Equivariant Graph Variational Autoencoders (MGVAE), the first hierarchical generative model to learn and generate graphs in a multiresolution and equivariant manner. At each resolution level, MGVAE employs higher order message passing to encode the graph while learning to partition it into mutually exclusive clusters and coarsening into a lower resolution that eventually creates a hierarchy of latent distributions. MGVAE then constructs a hierarchical generative model to variationally decode into a hierarchy of coarsened graphs. Importantly, our proposed framework is end-to-end permutation equivariant with respect to node ordering. MGVAE achieves competitive results with several generative tasks including general graph generation, molecular generation, unsupervised molecular representation learning to predict molecular properties, link prediction on citation graphs, and graph-based image generation.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Hy, Truong Son and Kondor, Risi},
	month = jun,
	year = {2022},
	note = {arXiv:2106.00967 [physics]},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Physics - Chemical Physics},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/S7MK9UGD/Hy and Kondor - 2022 - Multiresolution Equivariant Graph Variational Auto.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/K3DNNRIR/2106.html:text/html},
}


@article{hennequin_spleeter_2020,
	title = {Spleeter: a fast and efficient music source separation tool with pre-trained models},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {Spleeter},
	url = {https://joss.theoj.org/papers/10.21105/joss.02154},
	doi = {10.21105/joss.02154},
	abstract = {Hennequin et al., (2020). Spleeter: a fast and efficient music source separation tool with pre-trained models. Journal of Open Source Software, 5(50), 2154, https://doi.org/10.21105/joss.02154},
	language = {en},
	number = {50},
	urldate = {2023-07-03},
	journal = {Journal of Open Source Software},
	author = {Hennequin, Romain and Khlif, Anis and Voituret, Felix and Moussallam, Manuel},
	month = jun,
	year = {2020},
	pages = {2154},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/MXS48YXT/Hennequin et al. - 2020 - Spleeter a fast and efficient music source separa.pdf:application/pdf},
}


@misc{simon_learning_2018,
	title = {Learning a {Latent} {Space} of {Multitrack} {Measures}},
	url = {http://arxiv.org/abs/1806.00195},
	doi = {10.48550/arXiv.1806.00195},
	abstract = {Discovering and exploring the underlying structure of multi-instrumental music using learning-based approaches remains an open problem. We extend the recent MusicVAE model to represent multitrack polyphonic measures as vectors in a latent space. Our approach enables several useful operations such as generating plausible measures from scratch, interpolating between measures in a musically meaningful way, and manipulating specific musical attributes. We also introduce chord conditioning, which allows all of these operations to be performed while keeping harmony fixed, and allows chords to be changed while maintaining musical "style". By generating a sequence of measures over a predefined chord progression, our model can produce music with convincing long-term structure. We demonstrate that our latent space model makes it possible to intuitively control and generate musical sequences with rich instrumentation (see https://goo.gl/s2N7dV for generated audio).},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Simon, Ian and Roberts, Adam and Raffel, Colin and Engel, Jesse and Hawthorne, Curtis and Eck, Douglas},
	month = jun,
	year = {2018},
	note = {arXiv:1806.00195 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/ZC3HPUSE/Simon et al. - 2018 - Learning a Latent Space of Multitrack Measures.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/RZ5ICALL/1806.html:text/html},
}


@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/2VJY3MHF/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf},
}


@inproceedings{thul_relation_2008,
	address = {New York, NY, USA},
	series = {{C3S2E} '08},
	title = {On the relation between rhythm complexity measures and human rhythmic performance},
	isbn = {978-1-60558-101-9},
	url = {https://dl.acm.org/doi/10.1145/1370256.1370289},
	doi = {10.1145/1370256.1370289},
	abstract = {Six measures of musical rhythm complexity were compared experimentally to human difficulty of performance (performance complexity) using two data sets of rhythms, via phylogenetic trees of the rank-correlation coefficient matrices obtained from rankings of the rhythms according to the complexity measures. The results suggest the hypothesis that measures of rhythmic syncopation that are based on a weighted metrical hierarchy, are better predictors of human performance difficulty than measures based on cognitive complexity, weighted distances from onsets to beats, or mathematical irregularity.},
	urldate = {2023-07-02},
	booktitle = {Proceedings of the 2008 {C3S2E} conference},
	publisher = {Association for Computing Machinery},
	author = {Thul, Eric and Toussaint, Godfried T.},
	month = may,
	year = {2008},
	pages = {199--204},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/QX86B9GV/Thul and Toussaint - 2008 - On the relation between rhythm complexity measures.pdf:application/pdf},
}

@article{boulez_timbre_1987,
	title = {Timbre and composition ‐ timbre and language},
	volume = {2},
	issn = {0749-4467},
	url = {https://doi.org/10.1080/07494468708567057},
	doi = {10.1080/07494468708567057},
	abstract = {The function of timbre in 20th Century instrumental music is discussed in terms of the relation between timbre and musical language. Up to the 19th Century, the function of timbre was primarily related to its identity in addition to being charged with certain effective and symbolic characteristics. The identities of Western instruments are standardized, investing them with a certain neutrality that allows the construction of pitch hierarchies that are unperturbed by differences in timbre of the instruments. For small ensembles, timbre has a stability and a separating power that provides an identification and clarification of form and timbre. The function of timbral identity is one of articulation. With the modern orchestra, the use of instruments is more flexible and their identification becomes more mobile and temporary. Composing the blending of timbres into complex sound‐objects follows from the confronting of established sound hierarchies and an enriching of the sound vocabulary. The function of timbre in composed sound‐objects is one of illusion and is based upon the technique of fusion. One contrasts, then, notions of raw timbre and organized timbre. The importance of discontinuity in musical dimensions for composition is considered with respect to timbre. Finally, the relation between timbre, composition and sound transmission in an acoustic space is discussed.},
	number = {1},
	urldate = {2023-07-02},
	journal = {Contemporary Music Review},
	author = {Boulez, Pierre},
	month = jan,
	year = {1987},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/07494468708567057},
	keywords = {articulation, composition, fusion, identity, illusion, musical language, timbre},
	pages = {161--171},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/GK8KGZ5Z/Boulez - 1987 - Timbre and composition ‐ timbre and language.pdf:application/pdf},
}

@article{lubart_how_2005,
	title = {How can computers be partners in the creative process: classification and commentary on the special issue},
	volume = {63},
	issn = {1071-5819},
	shorttitle = {How can computers be partners in the creative process},
	url = {https://doi.org/10.1016/j.ijhcs.2005.04.002},
	doi = {10.1016/j.ijhcs.2005.04.002},
	abstract = {The different ways that computers can be involed in creative work are examined. A classification based on four categories of human-computer interaction to promote creativity is proposed: computers may facilitate (a) the management of creative work, (b) communication between individuals collaborating on creative projects, (c) the use of creativity enhancement techniques, (d) the creative act through integrated human-computer cooperation during idea production. The papers in the Special Issue are discussed according to this classification. Issues to be considered in future work on human-computer interactions for promoting creativity are discussed.},
	number = {4-5},
	urldate = {2023-07-02},
	journal = {International Journal of Human-Computer Studies},
	author = {Lubart, Todd},
	month = oct,
	year = {2005},
	keywords = {creativity, human-computer interaction},
	pages = {365--369},
}

@incollection{clark_grounding_1991,
	address = {Washington, DC, US},
	title = {Grounding in communication},
	isbn = {978-1-55798-121-9},
	abstract = {grounding [the process by which conversants try to establish that what has been said is understood] is so basic to communication . . . that it is important to understand how it works / take up two main factors that shape it / one is purpose—what the two people are trying to accomplish in their communication / the other is the medium of communication—the techniques available in the medium for accomplishing that purpose, and what it costs to use them  begin by briefly describing grounding as it appears in casual face-to-face conversation / then consider how it gets shaped by other purposes and in other media (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	booktitle = {Perspectives on socially shared cognition},
	publisher = {American Psychological Association},
	author = {Clark, Herbert H. and Brennan, Susan E.},
	year = {1991},
	doi = {10.1037/10096-006},
	keywords = {Communication Skills, Conversation, Verbal Comprehension},
	pages = {127--149},
	file = {Snapshot:/Users/admin 1/Zotero/storage/FHJFS2NP/1991-98452-006.html:text/html},
}

@article{rivero_classical_2020,
	title = {Classical {Music} {Prediction} and {Composition} by {Means} of {Variational} {Autoencoders}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/9/3053},
	doi = {10.3390/app10093053},
	abstract = {This paper proposes a new model for music prediction based on Variational Autoencoders (VAEs). In this work, VAEs are used in a novel way to address two different issues: music representation into the latent space, and using this representation to make predictions of the future note events of the musical piece. This approach was trained with different songs of Handel. As a result, the system can represent the music in the latent space, and make accurate predictions. Therefore, the system can be used to compose new music either from an existing piece or from a random starting point. An additional feature of this system is that a small dataset was used for training. However, results show that the system is able to return accurate representations and predictions on unseen data.},
	language = {en},
	number = {9},
	urldate = {2023-07-02},
	journal = {Applied Sciences},
	author = {Rivero, Daniel and Ramírez-Morales, Iván and Fernandez-Blanco, Enrique and Ezquerra, Norberto and Pazos, Alejandro},
	month = jan,
	year = {2020},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, music composition, variational autoencoders},
	pages = {3053},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/BR37PP63/Rivero et al. - 2020 - Classical Music Prediction and Composition by Mean.pdf:application/pdf},
}

@misc{sturm_music_2016,
	title = {Music transcription modelling and composition using deep learning},
	url = {http://arxiv.org/abs/1604.08723},
	doi = {10.48550/arXiv.1604.08723},
	abstract = {We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: {\textbackslash}url\{https://github.com/IraKorshunova/folk-rnn\}.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Sturm, Bob L. and Santos, João Felipe and Ben-Tal, Oded and Korshunova, Iryna},
	month = apr,
	year = {2016},
	note = {arXiv:1604.08723 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/MQRCCCY4/Sturm et al. - 2016 - Music transcription modelling and composition usin.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/VNLKML34/1604.html:text/html},
}

@misc{pati_attribute-based_2020,
	title = {Attribute-based {Regularization} of {Latent} {Spaces} for {Variational} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/2004.05485},
	doi = {10.48550/arXiv.2004.05485},
	abstract = {Selective manipulation of data attributes using deep generative models is an active area of research. In this paper, we present a novel method to structure the latent space of a Variational Auto-Encoder (VAE) to encode different continuous-valued attributes explicitly. This is accomplished by using an attribute regularization loss which enforces a monotonic relationship between the attribute values and the latent code of the dimension along which the attribute is to be encoded. Consequently, post-training, the model can be used to manipulate the attribute by simply changing the latent code of the corresponding regularized dimension. The results obtained from several quantitative and qualitative experiments show that the proposed method leads to disentangled and interpretable latent spaces that can be used to effectively manipulate a wide range of data attributes spanning image and symbolic music domains.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Pati, Ashis and Lerch, Alexander},
	month = jul,
	year = {2020},
	note = {arXiv:2004.05485 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/GRWWW7XN/Pati and Lerch - 2020 - Attribute-based Regularization of Latent Spaces fo.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/6YTJ46JG/2004.html:text/html},
}

@article{noel-hirst_autoethnographic_2023,
	title = {An {Autoethnographic} {Exploration} of {XAI} in {Algorithmic} {Composition}},
	url = {https://xaixarts.github.io/accepted-2023/Noel-Hirst-XAIxArts-2023-Paper.pdf},
	language = {en},
	journal = {in the 1st International Workshop on Explainable AI for the Arts (XAIxArts), ACM Creativity and Cognition (C\&C)},
	author = {Noel-Hirst, Ashley and Bryan-Kinns, Nick},
	year = {2023},
	pages = {3},
	file = {Noel-Hirst and Bryan-Kinns - An Autoethnographic Exploration of XAI in Algorith.pdf:/Users/admin 1/Zotero/storage/TSI8SJ98/Noel-Hirst and Bryan-Kinns - An Autoethnographic Exploration of XAI in Algorith.pdf:application/pdf},
}

@misc{noauthor_concept_nodate,
	title = {Concept whitening for interpretable image recognition {\textbar} {Nature} {Machine} {Intelligence}},
	url = {https://www.nature.com/articles/s42256-020-00265-z},
	urldate = {2023-07-02},
}

@article{chen_concept_2020,
	title = {Concept whitening for interpretable image recognition},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00265-z},
	doi = {10.1038/s42256-020-00265-z},
	abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can be misleading, unusable or rely on the latent space to possess properties that it may not have. Here, rather than attempting to analyse a neural network post hoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a convolutional neural network, the latent space is whitened (that is, decorrelated and normalized) and the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us with a much clearer understanding of how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens), the latent space. CW can be used in any layer of the network without hurting predictive performance.},
	language = {en},
	number = {12},
	urldate = {2023-07-02},
	journal = {Nature Machine Intelligence},
	author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
	month = dec,
	year = {2020},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Computer science, Statistics},
	pages = {772--782},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/REAUZKR2/Chen et al. - 2020 - Concept whitening for interpretable image recognit.pdf:application/pdf},
}

@article{fiebrink_meta-instrument_2009,
	title = {A {Meta}-{Instrument} for {Interactive}, {On}-the-{Fly} {Machine} {Learning}},
	abstract = {Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance.},
	journal = {NIME09},
	author = {Fiebrink, Rebecca and Trueman, Dan and Cook, Perry},
	month = jan,
	year = {2009},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/R8WS9IC5/Fiebrink et al. - 2009 - A Meta-Instrument for Interactive, On-the-Fly Mach.pdf:application/pdf},
}

@inproceedings{akten_learning_2019,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '19},
	title = {Learning to see: you are what you see},
	isbn = {978-1-4503-6311-2},
	shorttitle = {Learning to see},
	url = {https://dl.acm.org/doi/10.1145/3306211.3320143},
	doi = {10.1145/3306211.3320143},
	abstract = {The authors present a visual instrument developed as part of the creation of the artwork Learning to See. The artwork explores bias in artificial neural networks and provides mechanisms for the manipulation of specifically trained-for real-world representations. The exploration of these representations acts as a metaphor for the process of developing a visual understanding and/or visual vocabulary of the world. These representations can be explored and manipulated in real time, and have been produced in such a way so as to reflect specific creative perspectives that call into question the relationship between how both artificial neural networks and humans may construct meaning.},
	urldate = {2023-07-02},
	booktitle = {{ACM} {SIGGRAPH} 2019 {Art} {Gallery}},
	publisher = {Association for Computing Machinery},
	author = {Akten, Memo and Fiebrink, Rebecca and Grierson, Mick},
	month = jul,
	year = {2019},
	pages = {1--6},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/2CDW4ZAQ/Akten et al. - 2019 - Learning to see you are what you see.pdf:application/pdf},
}

@article{mcfee_software_nodate,
	title = {A {SOFTWARE} {FRAMEWORK} {FOR} {MUSICAL} {DATA} {AUGMENTATION}},
	abstract = {Predictive models for music annotation tasks are practically limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of “data augmentation” — supplementing a training set with carefully perturbed samples — has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated perturbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals.},
	language = {en},
	author = {McFee, Brian and Humphrey, Eric J and Bello, Juan P},
	file = {McFee et al. - A SOFTWARE FRAMEWORK FOR MUSICAL DATA AUGMENTATION.pdf:/Users/admin 1/Zotero/storage/8PF2M4M8/McFee et al. - A SOFTWARE FRAMEWORK FOR MUSICAL DATA AUGMENTATION.pdf:application/pdf},
}

@article{khandelwal_leveraging_2023,
	title = {Leveraging {Audio}-{Tagging} {Assisted} {Sound} {Event} {Detection} using {Weakified} {Strong} {Labels} and {Frequency} {Dynamic} {Convolutions}},
	journal = {arXiv preprint arXiv:2304.12688},
	author = {Khandelwal, Tanmay and Das, Rohan Kumar and Koh, Andrew and Chng, Eng Siong},
	year = {2023},
	file = {Full Text:/Users/admin 1/Zotero/storage/6AWB6WCU/Khandelwal et al. - 2023 - Leveraging Audio-Tagging Assisted Sound Event Dete.pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/TIGELK74/2304.html:text/html},
}

@article{chen_neural_2023,
	title = {Neural {Moderation} of {ASMR} {Erotica} {Content} in {Social} {Networks}},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Chen, Yixin and Jiang, Di and Tan, Conghui and Song, Yuanfeng and Zhang, Chen and Chen, Lei},
	year = {2023},
	note = {Publisher: IEEE},
	file = {Snapshot:/Users/admin 1/Zotero/storage/SVTZBFJG/10145599.html:text/html},
}

@phdthesis{ramires_automatic_2023,
	title = {Automatic characterization and generation of music loops and instrument samples for electronic music production pp37-41},
	school = {Universitat Pompeu Fabra},
	author = {Ramires, António},
	year = {2023},
	note = {Publisher: Universitat Pompeu Fabra},
	file = {Full Text:/Users/admin 1/Zotero/storage/QQIZJB3K/Ramires - 2023 - Automatic characterization and generation of music.pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/524XIGS7/687697.pdf:application/pdf},
}

@phdthesis{wang_adaptive_2023,
	type = {{PhD} {Thesis}},
	title = {Adaptive and {Interactive} {Machine} {Listening} with {Minimal} {Supervision}},
	school = {New York University},
	author = {Wang, Yu},
	year = {2023},
	file = {Snapshot:/Users/admin 1/Zotero/storage/H26VBDJI/intermediateredirectforezproxy.html:text/html;Wang - 2023 - Adaptive and Interactive Machine Listening with Mi.pdf:/Users/admin 1/Zotero/storage/SKNFYCY8/Wang - 2023 - Adaptive and Interactive Machine Listening with Mi.pdf:application/pdf},
}

@phdthesis{chiantello_acoustic_2023,
	type = {{PhD} {Thesis}},
	title = {Acoustic modelling, data augmentation and feature extraction for in-pipe machine learning applications},
	school = {Manchester Metropolitan University},
	author = {Chiantello, Dario Alfredo},
	year = {2023},
	file = {Full Text:/Users/admin 1/Zotero/storage/NZBVWHZ3/Chiantello - 2023 - Acoustic modelling, data augmentation and feature .pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/S3XK76BQ/631518.html:text/html},
}

@article{bonafos_detecting_2023,
	title = {Detecting human and non-human vocal productions in large scale audio recordings},
	journal = {arXiv preprint arXiv:2302.07640},
	author = {Bonafos, Guillem and Pudlo, Pierre and Freyermuth, Jean-Marc and Legou, Thierry and Fagot, Joël and Tronçon, Samuel and Rey, Arnaud},
	year = {2023},
	file = {Full Text:/Users/admin 1/Zotero/storage/LN2KCLZW/Bonafos et al. - 2023 - Detecting human and non-human vocal productions in.pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/VIB6JW4I/2302.html:text/html},
}

@article{sanctorum_end-user_2022,
	title = {End-user engineering of ontology-based knowledge bases},
	volume = {41},
	issn = {0144-929X},
	url = {https://doi.org/10.1080/0144929X.2022.2092032},
	doi = {10.1080/0144929X.2022.2092032},
	abstract = {Knowledge bases store information on certain topics. Applying a well-structured and machine-readable format for a knowledge base is a prerequisite for any AI-based processing or reasoning. Semantic technologies (e.g. RDF) offer such a format and have the advantages that they make it possible to define the semantics of the information and support advanced querying. However, the disadvantage is that using such technologies is challenging for people not trained in IT, such as subject matter experts. This means that they need to rely on semantic technology experts to create, maintain, and query their knowledge bases. However, these experts are, in turn, not trained in the subject matter, while domain knowledge is essential for the construction of high-quality knowledge bases. In this paper, we present an end-user engineering approach for ontology-based knowledge bases. The goal is to allow subject matter experts to develop, maintain, and exploit the knowledge base themselves. We also present the supporting tools developed so far. The tools for the construction and the manual filling of the knowledge base are using the jigsaw metaphor to hide technicalities and guide the users. We also developed tools to automatically import data from spreadsheets into the knowledge base and to perform some type of quality control on the data. The end-user approach and the tools are demonstrated and evaluated for building a knowledge base in the toxicology domain.},
	number = {9},
	urldate = {2023-07-02},
	journal = {Behaviour \& Information Technology},
	author = {Sanctorum, Audrey and Riggio, Jonathan and Maushagen, Jan and Sepehri, Sara and Arnesdotter, Emma and Delagrange, Mona and De Kock, Joery and Vanhaecke, Tamara and Debruyne, Christophe and De Troyer, Olga},
	month = jul,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0144929X.2022.2092032},
	keywords = {data lifting, domain ontology development, end-user development, jigsaw metaphor, Knowledge base engineering, quality assurance},
	pages = {1811--1829},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/FGEVQANS/Sanctorum et al. - 2022 - End-user engineering of ontology-based knowledge b.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/HZJ6N8SR/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/7DNK4UGC/2005.html:text/html},
}

@incollection{jain_hugging_2022,
	address = {Berkeley, CA},
	title = {Hugging {Face}},
	isbn = {978-1-4842-8844-3},
	url = {https://doi.org/10.1007/978-1-4842-8844-3_4},
	abstract = {If you have even a passing familiarity with the advancements that have been made in the fields of machine learning and artificial intelligence in the years since 2018, you have almost certainly become aware of the tremendous strides that have been taken in the field of natural language processing (also known as NLP). Most of the progress in this area can be attributed to large language models, also known as LLMs. The architecture behind these LLMs is the transformer’s encoder-decoder, which we discussed in Chapter 2.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Introduction to {Transformers} for {NLP}: {With} the {Hugging} {Face} {Library} and {Models} to {Solve} {Problems}},
	publisher = {Apress},
	author = {Jain, Shashank Mohan},
	editor = {Jain, Shashank Mohan},
	year = {2022},
	doi = {10.1007/978-1-4842-8844-3_4},
	pages = {51--67},
}

@misc{noauthor_hugging_nodate,
	title = {Hugging {Face} – {The} {AI} community building the future.},
	url = {https://huggingface.co/},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2023-07-02},
	file = {Snapshot:/Users/admin 1/Zotero/storage/YCK2BUP5/huggingface.co.html:text/html},
}

@incollection{bisong_google_2019,
	address = {Berkeley, CA},
	title = {Google {Colaboratory}},
	isbn = {978-1-4842-4470-8},
	url = {https://doi.org/10.1007/978-1-4842-4470-8_7},
	abstract = {Google Colaboratory more commonly referred to as “Google Colab” or just simply “Colab” is a research project for prototyping machine learning models on powerful hardware options such as GPUs and TPUs. It provides a serverless Jupyter notebook environment for interactive development. Google Colab is free to use like other G Suite products.},
	language = {en},
	urldate = {2023-07-02},
	booktitle = {Building {Machine} {Learning} and {Deep} {Learning} {Models} on {Google} {Cloud} {Platform}: {A} {Comprehensive} {Guide} for {Beginners}},
	publisher = {Apress},
	author = {Bisong, Ekaba},
	editor = {Bisong, Ekaba},
	year = {2019},
	doi = {10.1007/978-1-4842-4470-8_7},
	pages = {59--64},
}

@misc{jiang_empirical_2023,
	title = {An {Empirical} {Study} of {Pre}-{Trained} {Model} {Reuse} in the {Hugging} {Face} {Deep} {Learning} {Model} {Registry}},
	url = {http://arxiv.org/abs/2303.02552},
	doi = {10.48550/arXiv.2303.02552},
	abstract = {Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems. In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Jiang, Wenxin and Synovic, Nicholas and Hyatt, Matt and Schorlemmer, Taylor R. and Sethi, Rohan and Lu, Yung-Hsiang and Thiruvathukal, George K. and Davis, James C.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.02552 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/GDRXTGU8/Jiang et al. - 2023 - An Empirical Study of Pre-Trained Model Reuse in t.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/VWJVEDHR/2303.html:text/html},
}

@inproceedings{ait_hfcommunity_2023,
	title = {{HFCommunity}: {A} {Tool} to {Analyze} the {Hugging} {Face} {Hub} {Community}},
	shorttitle = {{HFCommunity}},
	doi = {10.1109/SANER56733.2023.00080},
	abstract = {In recent years, empirical studies on software engineering practices have primarily relied on general-purpose social coding platforms such as GITHUB or GITLAB. With the emergence of Machine Learning (ML), platforms specifically designed for hosting and developing ML-based projects have appeared, being HUGGING FACE HUB one of the most popular ones. HUGGING FACE HUB focuses on facilitating the sharing of datasets, pre-trained ML models and applications built with them (spaces in HUGGING FACE HUB terminology). Besides, the Hub is adding more and more collaborative features, such as issues and pull requests, to facilitate the building of these artifacts within the platform itself. With over 100K repositories, and growing fast, HUGGING FACE HUB is therefore becoming a promising source of data on all aspects of ML projects and the community interactions around them. As such, we believe it is a promising source for all types of empirical studies aimed at analyzing the collaborative development and evolution of ML artifacts. Nevertheless, apart from the API provided by the platform, there are no easy-to-use solutions to collect and explore the different facets of HUGGING FACE HUB data, including the repositories, discussions and code evolution. To overcome this situation, in this paper we present HFCOMMUNITY, a relational database populated with HUGGING FACE HUB data to facilitate empirical analysis on the growing number of ML-related development projects.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Ait, Adem and Izquierdo, Javier Luis Cánovas and Cabot, Jordi},
	month = mar,
	year = {2023},
	note = {ISSN: 2640-7574},
	keywords = {Collaboration, Data Analysis, Data mining, Hugging Face, Matched filters, Mining Software Repositories, Relational databases, Scalability, Soft sensors, Terminology},
	pages = {728--732},
	file = {IEEE Xplore Abstract Record:/Users/admin 1/Zotero/storage/7PFLDE7T/10123660.html:text/html;IEEE Xplore Full Text PDF:/Users/admin 1/Zotero/storage/UEZ8VFRR/Ait et al. - 2023 - HFCommunity A Tool to Analyze the Hugging Face Hu.pdf:application/pdf},
}

@book{latour_reassembling_2007,
	title = {Reassembling the {Social}: {An} {Introduction} to {Actor}-{Network}-{Theory}},
	isbn = {978-0-19-162289-2},
	shorttitle = {Reassembling the {Social}},
	abstract = {Reassembling the Social is a fundamental challenge from one of the world's leading social theorists to how we understand society and the 'social'. Bruno Latour's contention is that the word 'social', as used by Social Scientists, has become laden with assumptions to the point where it has become misnomer. When the adjective is applied to a phenomenon, it is used to indicate a stablilized state of affairs, a bundle of ties that in due course may be used to account for another phenomenon. But Latour also finds the word used as if it described a type of material, in a comparable way to an adjective such as 'wooden' or 'steely'. Rather than simply indicating what is already assembled together, it is now used in a way that makes assumptions about the nature of what is assembled. It has become a word that designates two distinct things: a process of assembling; and a type of material, distinct from others. Latour shows why 'the social' cannot be thought of as a kind of material or domain, and disputes attempts to provide a 'social explanations' of other states of affairs. While these attempts have been productive (and probably necessary) in the past, the very success of the social sciences mean that they are largely no longer so. At the present stage it is no longer possible to inspect the precise constituents entering the social domain. Latour returns to the original meaning of 'the social' to redefine the notion, and allow it to trace connections again. It will then be possible to resume the traditional goal of the social sciences, but using more refined tools. Drawing on his extensive work examining the 'assemblages' of nature, Latour finds it necessary to scrutinize thoroughly the exact content of what is assembled under the umbrella of Society. This approach, a 'sociology of associations', has become known as Actor-Network-Theory, and this book is an essential introduction both for those seeking to understand Actor-Network Theory, or the ideas of one of its most influential proponents.},
	language = {en},
	publisher = {OUP Oxford},
	author = {Latour, Bruno},
	month = sep,
	year = {2007},
	note = {Google-Books-ID: BgJREAAAQBAJ},
	keywords = {Business \& Economics / Organizational Behavior, Philosophy / Political, Social Science / Sociology / General},
}

@article{latour_actor-network_1996,
	title = {On actor-network theory: {A} few clarifications},
	volume = {47},
	issn = {0038-6073},
	shorttitle = {On actor-network theory},
	url = {https://www.jstor.org/stable/40878163},
	abstract = {Three resources have been developed over the ages to deal with agency. The first one is to attribute to them naturality, and to link them with nature. The second one is to grant them sociality, and to tie them to the social fabric. The third one is to consider them as a semiotic construction, and to relate agency with building of meaning. The originality of science studies comes from the impossibility of clearly differentiating between these three resources. Microbes, neutrinos of DNA are at the same time natural, social and discourse. They are real, human and semiotic entities in the same breath. The article explores the consequences of this peculiar situation which has not been underlined before science studies forced us to retie the links between these three resources. The actor-network theory as developed by Callon and his colleagues is an attempt to invent a vocabulary to deal with this new situation. The article reviews those difficulties and tries to overcome them by showing how they may be used to account for the construction of entities, that is for the attribution of nature, society and meaning.},
	number = {4},
	urldate = {2023-07-03},
	journal = {Soziale Welt},
	author = {Latour, Bruno},
	year = {1996},
	note = {Publisher: Nomos Verlagsgesellschaft mbH},
	pages = {369--381},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/4UG4T5GJ/Latour - 1996 - On actor-network theory A few clarifications.pdf:application/pdf},
}

@article{latour_politics_1988,
	title = {The politics of explanation: {An} alternative},
	volume = {10},
	shorttitle = {The politics of explanation},
	journal = {Knowledge and reflexivity: New frontiers in the sociology of knowledge},
	author = {Latour, Bruno},
	year = {1988},
	note = {Publisher: Sage London},
	pages = {155--176},
	file = {Full Text:/Users/admin 1/Zotero/storage/GKWSRN6S/Latour - 1988 - The politics of explanation An alternative.pdf:application/pdf},
}

@article{magnusson_epistemic_2009,
	title = {Of {Epistemic} {Tools}: musical instruments as cognitive extensions},
	volume = {14},
	issn = {1469-8153, 1355-7718},
	shorttitle = {Of {Epistemic} {Tools}},
	url = {https://www.cambridge.org/core/journals/organised-sound/article/of-epistemic-tools-musical-instruments-as-cognitive-extensions/AFBB6209D3369A2DB14A0FDB71CBF611},
	doi = {10.1017/S1355771809000272},
	abstract = {This paper explores the differences in the design and performance of acoustic and new digital musical instruments, arguing that with the latter there is an increased encapsulation of musical theory. The point of departure is the phenomenology of musical instruments, which leads to the exploration of designed artefacts as extensions of human cognition – as scaffolding onto which we delegate parts of our cognitive processes. The paper succinctly emphasises the pronounced epistemic dimension of digital instruments when compared to acoustic instruments. Through the analysis of material epistemologies it is possible to describe the digital instrument as an epistemic tool: a designed tool with such a high degree of symbolic pertinence that it becomes a system of knowledge and thinking in its own terms. In conclusion, the paper rounds up the phenomenological and epistemological arguments, and points at issues in the design of digital musical instruments that are germane due to their strong aesthetic implications for musical culture.},
	language = {en},
	number = {2},
	urldate = {2023-07-03},
	journal = {Organised Sound},
	author = {Magnusson, Thor},
	month = aug,
	year = {2009},
	note = {Publisher: Cambridge University Press},
	pages = {168--176},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/YZY6NLP4/Magnusson - 2009 - Of Epistemic Tools musical instruments as cogniti.pdf:application/pdf},
}

@article{armitage_agential_nodate,
	title = {{AGENTIAL} {SCORES}: {EXPLORING} {EMERGENT}, {SELF}-{ORGANISING} {AND} {ENTANGLED} {MUSIC} {NOTATION}},
	abstract = {Dynamic scores have gained popularity as an innovative intervention in musical performance, providing novelty for both performers and audiences. In this paper, we discuss agential scores and the implications of their emergent, selforganising, and entanglement affordances for musical performance ecologies. We approach this through practicebased research, introducing To¨lvera, an artificial life software library for agential scores. We propose a typology of interaction scenarios for agential scores and investigate a subset of these, presenting the outcomes of early artistic encounters with To¨lvera between two improvising guitarists. Reflecting on our experiences, we emphasise the unique challenges that emerge from engaging with scores as real-time agents, suggesting that agential scores promote fluidity of form in notation, which consequently prompts performers to identify with, mirror, and attune to them. Although scores have always possessed agency, we argue that a more explicit and practical focus on agency raises new questions and offers new possibilities for the intraactions between human and non-human agents within musical ecologies.},
	language = {en},
	author = {Armitage, Jack},
	file = {Armitage - AGENTIAL SCORES EXPLORING EMERGENT, SELF-ORGANISI.pdf:/Users/admin 1/Zotero/storage/7D8W55UI/Armitage - AGENTIAL SCORES EXPLORING EMERGENT, SELF-ORGANISI.pdf:application/pdf},
}

@article{tahiroglu_digital_2020,
	title = {Digital {Musical} {Instruments} as {Probes}: {How} computation changes the mode-of-being of musical instruments},
	volume = {25},
	issn = {1355-7718, 1469-8153},
	shorttitle = {Digital {Musical} {Instruments} as {Probes}},
	url = {https://www.cambridge.org/core/journals/organised-sound/article/digital-musical-instruments-as-probes-how-computation-changes-the-modeofbeing-of-musical-instruments/9A008ADAFDBD45148339B9783C5B718F},
	doi = {10.1017/S1355771819000475},
	abstract = {This article explores how computation opens up possibilities for new musical practices to emerge through technology design. Using the notion of the cultural probe as a lens, we consider the digital musical instrument as an experimental device that yields findings across the fields of music, sociology and acoustics. As part of an artistic-research methodology, the instrumental object as a probe is offered as a means for artists to answer questions that are often formulated outside semantic language. This article considers how computation plays an important role in the authors’ personal performance practices in different ways, which reflect the changed mode-of-being of new musical instruments and our individual and collective relations with them.},
	language = {en},
	number = {1},
	urldate = {2023-07-03},
	journal = {Organised Sound},
	author = {Tahıroğlu, Koray and Magnusson, Thor and Parkinson, Adam and Garrelfs, Iris and Tanaka, Atau},
	month = apr,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	pages = {64--74},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/ENV25T7D/Tahıroğlu et al. - 2020 - Digital Musical Instruments as Probes How computa.pdf:application/pdf},
}

@article{privato_context-sensitive_2023,
	title = {A {Context}-{Sensitive} {Approach} to {XAI} in {Music} {Performance}},
	language = {en},
	journal = {in the 1st International Workshop on Explainable AI for the Arts (XAIxArts), ACM Creativity and Cognition (C\&C)},
	author = {Privato, Nicola and Armitage, Jack},
	year = {2023},
	pages = {3},
	file = {Privato and Armitage - A Context-Sensitive Approach to XAI in Music Perfo.pdf:/Users/admin 1/Zotero/storage/A8MACK98/Privato and Armitage - A Context-Sensitive Approach to XAI in Music Perfo.pdf:application/pdf},
}

@article{nyrup_explanatory_2022,
	title = {Explanatory pragmatism: a context-sensitive framework for explainable medical {AI}},
	volume = {24},
	issn = {1388-1957},
	shorttitle = {Explanatory pragmatism},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8885497/},
	doi = {10.1007/s10676-022-09632-3},
	abstract = {Explainable artificial intelligence (XAI) is an emerging, multidisciplinary field of research that seeks to develop methods and tools for making AI systems more explainable or interpretable. XAI researchers increasingly recognise explainability as a context-, audience- and purpose-sensitive phenomenon, rather than a single well-defined property that can be directly measured and optimised. However, since there is currently no overarching definition of explainability, this poses a risk of miscommunication between the many different researchers within this multidisciplinary space. This is the problem we seek to address in this paper. We outline a framework, called Explanatory Pragmatism, which we argue has two attractive features. First, it allows us to conceptualise explainability in explicitly context-, audience- and purpose-relative terms, while retaining a unified underlying definition of explainability. Second, it makes visible any normative disagreements that may underpin conflicting claims about explainability regarding the purposes for which explanations are sought. Third, it allows us to distinguish several dimensions of AI explainability. We illustrate this framework by applying it to a case study involving a machine learning model for predicting whether patients suffering disorders of consciousness were likely to recover consciousness.},
	number = {1},
	urldate = {2023-07-03},
	journal = {Ethics and Information Technology},
	author = {Nyrup, Rune and Robinson, Diana},
	year = {2022},
	pmid = {35250370},
	pmcid = {PMC8885497},
	pages = {13},
	file = {PubMed Central Full Text PDF:/Users/admin 1/Zotero/storage/A4CAQC73/Nyrup and Robinson - 2022 - Explanatory pragmatism a context-sensitive framew.pdf:application/pdf},
}

@misc{noauthor_music_nodate,
	title = {Music and {Artificial} {Intelligence}: {Building} {Critical} {Interdisciplinary} {Studies} {\textbar} {MusAI} {Project} {\textbar} {Fact} {Sheet} {\textbar} {H2020}},
	shorttitle = {Music and {Artificial} {Intelligence}},
	url = {https://cordis.europa.eu/project/id/101019164},
	abstract = {Recent years have seen escalating concern about the impacts of artificial intelligence (AI). While growing academic and policy literatures address the social and ethical implications of AI, as yet no major research initiative examines AI’s cultural implications. Music has...},
	language = {en},
	urldate = {2023-07-03},
	journal = {CORDIS {\textbar} European Commission},
	file = {Snapshot:/Users/admin 1/Zotero/storage/4MCRLWVI/101019164.html:text/html},
}

@inproceedings{datta_algorithmic_2016,
	title = {Algorithmic {Transparency} via {Quantitative} {Input} {Influence}: {Theory} and {Experiments} with {Learning} {Systems}},
	shorttitle = {Algorithmic {Transparency} via {Quantitative} {Input} {Influence}},
	doi = {10.1109/SP.2016.42},
	abstract = {Algorithmic systems that employ machine learning play an increasing role in making substantive decisions in modern society, ranging from online personalization to insurance and credit decisions to predictive policing. But their decision-making processes are often opaque-it is difficult to explain why a certain decision was made. We develop a formal foundation to improve the transparency of such decision-making systems. Specifically, we introduce a family of Quantitative Input Influence (QII) measures that capture the degree of influence of inputs on outputs of systems. These measures provide a foundation for the design of transparency reports that accompany system decisions (e.g., explaining a specific credit decision) and for testing tools useful for internal and external oversight (e.g., to detect algorithmic discrimination). Distinctively, our causal QII measures carefully account for correlated inputs while measuring influence. They support a general class of transparency queries and can, in particular, explain decisions about individuals (e.g., a loan decision) and groups (e.g., disparate impact based on gender). Finally, since single inputs may not always have high influence, the QII measures also quantify the joint influence of a set of inputs (e.g., age and income) on outcomes (e.g. loan decisions) and the marginal influence of individual inputs within such a set (e.g., income). Since a single input may be part of multiple influential sets, the average marginal influence of the input is computed using principled aggregation measures, such as the Shapley value, previously applied to measure influence in voting. Further, since transparency reports could compromise privacy, we explore the transparency-privacy tradeoff and prove that a number of useful transparency reports can be made differentially private with very little addition of noise. Our empirical validation with standard machine learning algorithms demonstrates that QII measures are a useful transparency mechanism when black box access to the learning system is available. In particular, they provide better explanations than standard associative measures for a host of scenarios that we consider. Further, we show that in the situations we consider, QII is efficiently approximable and can be made differentially private while preserving accuracy.},
	booktitle = {2016 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Datta, Anupam and Sen, Shayak and Zick, Yair},
	month = may,
	year = {2016},
	note = {ISSN: 2375-1207},
	keywords = {Algorithm design and analysis, Atmospheric measurements, Correlation, Decision making, fairness, machine learning, Machine learning algorithms, Particle measurements, Privacy, transparency},
	pages = {598--617},
	file = {IEEE Xplore Abstract Record:/Users/admin 1/Zotero/storage/4HZSDCBK/7546525.html:text/html;IEEE Xplore Full Text PDF:/Users/admin 1/Zotero/storage/72PW7G93/Datta et al. - 2016 - Algorithmic Transparency via Quantitative Input In.pdf:application/pdf},
}

@misc{jonason_audio_2022,
	title = {Audio {Latent} {Space} {Cartography}},
	url = {http://arxiv.org/abs/2212.02610},
	doi = {10.48550/arXiv.2212.02610},
	abstract = {We explore the generation of visualisations of audio latent spaces using an audio-to-image generation pipeline. We believe this can help with the interpretability of audio latent spaces. We demonstrate a variety of results on the NSynth dataset. A web demo is available.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Jonason, Nicolas and Sturm, Bob L. T.},
	month = dec,
	year = {2022},
	note = {arXiv:2212.02610 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, J.5},
	file = {arXiv Fulltext PDF:/Users/admin 1/Zotero/storage/6LNDYJBS/Jonason and Sturm - 2022 - Audio Latent Space Cartography.pdf:application/pdf;arXiv.org Snapshot:/Users/admin 1/Zotero/storage/37LH92FP/2212.html:text/html},
}

@article{clemens_explaining_2023,
	title = {Explaining the {Arts}: {Toward} a {Framework} for {Matching} {Creative} {Tasks} with {Appropriate} {Explanation} {Mediums}},
	language = {en},
	journal = {in the 1st International Workshop on Explainable AI for the Arts (XAIxArts), ACM Creativity and Cognition (C\&C)},
	author = {Clemens, Michael},
	year = {2023},
	pages = {3},
	file = {Clemens - Explaining the Arts Toward a Framework for Matchi.pdf:/Users/admin 1/Zotero/storage/Q7PHGPD5/Clemens - Explaining the Arts Toward a Framework for Matchi.pdf:application/pdf},
}

@article{vigliensoni_interacting_2023,
	title = {Interacting with neural audio synthesis models through interactive machine learning},
	abstract = {Recent advances in neural audio synthesis have made it possible to generate audio signals in real time, enabling the use of applications in musical performance. However, exploring and playing with their high-dimensional spaces remains challenging, as the axes do not necessarily correlate to clear musical labels and may vary from model to model. In this paper, we present a proof-ofconcept mechanism for steering latent audio models through interactive machine learning. Our approach involves mapping the human-performance space to the high-dimensional, computergenerated latent space of a neural audio model by utilizing a regressive model learned from a set of demonstrative actions. By implementing this method in ideation, exploration, and sound and music performance we have observed its ef ciency, exibility, and immediacy of control over generative audio processes.},
	language = {en},
	journal = {in the 1st International Workshop on Explainable AI for the Arts (XAIxArts), ACM Creativity and Cognition (C\&C)},
	author = {Vigliensoni, Gabriel and Fiebrink, Rebecca},
	year = {2023},
	file = {Vigliensoni and Fiebrink - Interacting with neural audio synthesis models thr.pdf:/Users/admin 1/Zotero/storage/NSBJ4P4Q/Vigliensoni and Fiebrink - Interacting with neural audio synthesis models thr.pdf:application/pdf},
}

@article{vigliensoniRVAELiveLatent,
  title = {R-{{VAE}}: {{Live}} Latent Space Drum Rhythm Generation from Minimal-Size Datasets},
  author = {Vigliensoni, Gabriel and McCallum, Louis and Maestre, Esteban},
  pages = {25},
  abstract = {In this article, we present R-VAE, a system designed for the modeling and exploration of latent spaces learned from rhythms encoded in MIDI clips. The system is based on a variational autoencoder neural network, uses a data structure that is capable of encoding rhythms in simple and compound meter, and can learn models from little training data. To facilitate the exploration of models, we implemented a visualizer that relies on the dynamic nature of the pulsing rhythmic patterns. To test our system in real-life musical practice, we collected small-scale datasets of contemporary music genre rhythms and trained models with them. We found that the non-linearities of the learned latent spaces coupled with tactile interfaces to interact with the models were very expressive and led to unexpected places in musical composition and live performance settings. A music album was recorded and it was premiered at a major music festival using the VAE latent space on stage.},
  langid = {english},
  file = {/Users/ash/Zotero/storage/VWMYJS3H/Vigliensoni et al. - R-VAE Live latent space drum rhythm generation fr.pdf}
}


@inproceedings{prpa_articulating_2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Articulating {Experience}: {Reflections} from {Experts} {Applying} {Micro}-{Phenomenology} to {Design} {Research} in {HCI}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Articulating {Experience}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376664},
	doi = {10.1145/3313831.3376664},
	abstract = {Third wave HCI initiated a slow transformation in the methods of UX research: from widely used quantitative approaches to more recently employed qualitative techniques. Articulating the nuances, complexity, and diversity of a user's experience beyond surface descriptions remains a challenge within design. One qualitative method — micro-phenomenology — has been used in HCI/Design research since 2001. Yet, no systematic understanding of micro-phenomenology has been presented, particularly from the perspective of HCI/Design researchers who actively use it in design contexts. We interviewed 5 HCI/Design experts who utilize micro-phenomenology and present their experiences with the method. We illustrate how this method has been applied by the selected experts through developing a practice, and present conditions under which the descriptions of the experience unfold, and the values that this method can provide to HCI/Design field. Our contribution highlights the value of micro-phenomenology in articulating the experience of designers and participants, developing vocabulary for multi-sensory experiences, and unfolding embodied tacit knowledge.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Prpa, Mirjana and Fdili-Alaoui, Sarah and Schiphorst, Thecla and Pasquier, Philippe},
	month = apr,
	year = {2020},
	keywords = {empirical methods, micro-phenomenology, stage 0 important, user experience},
	pages = {1--14},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/PMUVNI22/Prpa et al. - 2020 - Articulating Experience Reflections from Experts .pdf:application/pdf},
}

@inproceedings{reed_exploring_2022,
	title = {Exploring {Experiences} with {New} {Musical} {Instruments} through {Micro}-phenomenology},
	url = {https://nime.pubpub.org/pub/o7sza3sq/release/1},
	doi = {10.21428/92fbeb44.b304e4b1},
	abstract = {This paper introduces micro-phenomenology, a research discipline for exploring and uncovering the structures of lived experience, as a beneficial methodology for studying and evaluating interactions with digital musical instruments. Compared to other subjective methods, micro-phenomenology evokes and returns one to the moment of experience, allowing access to dimensions and observations which may not be recalled in reflection alone. We present a case study of five micro-phenomenological interviews conducted with musicians about their experiences with existing digital musical instruments. The interviews reveal deep, clear descriptions of different modalities of synchronic moments in interaction, especially in tactile connections and bodily sensations. We highlight the elements of interaction captured in these interviews which would not have been revealed otherwise and the importance of these elements in researching perception, understanding, interaction, and performance with digital musical instruments.},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {International {Conference} on {New} {Interfaces} for {Musical} {Expression}},
	author = {Reed, Courtney N. and Nordmoen, Charlotte and Martelloni, Andrea and Lepri, Giacomo and Robson, Nicole and Zayas-Garin, Eevee and Cotton, Kelsey and Mice, Lia and McPherson, Andrew},
	month = jun,
	year = {2022},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/DB8R88ED/Reed et al. - 2022 - Exploring Experiences with New Musical Instruments.pdf:application/pdf},
}

@book{schaeffer_search_2012,
	title = {In {Search} of a {Concrete} {Music}},
	copyright = {Available worldwide},
	isbn = {978-0-520-26574-5},
	abstract = {Pierre Schaeffer’s In Search of a Concrete Music (À la recherche d’une musique concrète) has long been considered a classic text in electroacoustic music and sound recording. Now Schaeffer’s pioneering work—at once a journal of his experiments in sound composition and a treatise on the raison d’être of “concrete music”—is available for the first time in English translation. Schaeffer’s theories have had a profound influence on composers working with technology. However, they extend beyond the confines of the studio and are applicable to many areas of contemporary musical thought, such as defining an ‘instrument’ and classifying sounds. Schaeffer has also become increasingly relevant to DJs and hip-hop producers as well as sound-based media artists. This unique book is essential for anyone interested in contemporary musicology or media history.},
	language = {en},
	urldate = {2023-07-03},
	author = {Schaeffer, Pierre},
	translator = {Dack, John and North, Christine},
	month = nov,
	year = {2012},
}

@misc{noauthor_rosalis_nodate,
	title = {{ROSALÍA}'s '{CUUUUuuuuuute}' sample of {Soytiet}'s '{Numbers}'},
	url = {https://www.whosampled.com/sample/932259/ROSAL%C3%8DA-CUUUUuuuuuute-Soytiet-Numbers/},
	abstract = {"CUUUUuuuuuute" by ROSALÍA contains a sample of another track. Listen to both tracks on WhoSampled, the ultimate database of sampled music, cover songs and remixes.},
	language = {en},
	urldate = {2023-07-03},
	journal = {WhoSampled},
	file = {Snapshot:/Users/admin 1/Zotero/storage/C3YKTE5T/ROSALÍA-CUUUUuuuuuute-Soytiet-Numbers.html:text/html},
}

@misc{noauthor_8_nodate,
	title = {8 {Times} {J} {Dilla} {Changed} the {Meaning} of a {Sample} — {Unspotted}},
	url = {https://www.unspottedmusic.com/music/dilla},
	urldate = {2023-07-03},
	file = {8 Times J Dilla Changed the Meaning of a Sample — Unspotted:/Users/admin 1/Zotero/storage/9FLP5K5P/dilla.html:text/html},
}

@inproceedings{kiela_hateful_2020,
	title = {The {Hateful} {Memes} {Challenge}: {Detecting} {Hate} {Speech} in {Multimodal} {Memes}},
	volume = {33},
	shorttitle = {The {Hateful} {Memes} {Challenge}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1b84c4cee2b8b3d823b30e2d604b1878-Abstract.html},
	abstract = {This work proposes a new challenge set for multimodal classification, focusing on
detecting hate speech in multimodal memes. It is constructed such that unimodal
models struggle and only multimodal models can succeed: difficult examples
(“benign confounders”) are added to the dataset to make it hard to rely on unimodal
signals. The task requires subtle reasoning, yet is straightforward to evaluate
as a binary classification problem. We provide baseline performance numbers
for unimodal models, as well as for multimodal models with various degrees of
sophistication. We find that state-of-the-art methods perform poorly compared to
humans, illustrating the difficulty of the task and highlighting the challenge that this important problem poses to the community.},
	urldate = {2023-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
	year = {2020},
	pages = {2611--2624},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/A5Z3IXEP/Kiela et al. - 2020 - The Hateful Memes Challenge Detecting Hate Speech.pdf:application/pdf},
}

@incollection{bown_sociocultural_2021,
	address = {Cham},
	title = {Sociocultural and {Design} {Perspectives} on {AI}-{Based} {Music} {Production}: {Why} {Do} {We} {Make} {Music} and {What} {Changes} if {AI} {Makes} {It} for {Us}?},
	isbn = {978-3-030-72116-9},
	shorttitle = {Sociocultural and {Design} {Perspectives} on {AI}-{Based} {Music} {Production}},
	url = {https://doi.org/10.1007/978-3-030-72116-9_1},
	abstract = {The recent advance of artificial intelligence (AI) technologies that can generate musical material.},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {Handbook of {Artificial} {Intelligence} for {Music}: {Foundations}, {Advanced} {Approaches}, and {Developments} for {Creativity}},
	publisher = {Springer International Publishing},
	author = {Bown, Oliver},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9_1},
	pages = {1--20},
}

@inproceedings{manco_learning_2022,
	title = {Learning {Music} {Audio} {Representations} {Via} {Weak} {Language} {Supervision}},
	doi = {10.1109/ICASSP43922.2022.9746996},
	abstract = {Audio representations for music information retrieval are typically learned via supervised learning in a task-specific fashion. Although effective at producing state-of-the-art results, this scheme lacks flexibility with respect to the range of applications a model can have and requires extensively annotated datasets. In this work, we pose the question of whether it may be possible to exploit weakly aligned text as the only supervisory signal to learn general-purpose music audio representations. To address this question, we design a multimodal architecture for music and language pre-training (MuLaP) optimised via a set of proxy tasks. Weak supervision is provided in the form of noisy natural language descriptions conveying the overall musical content of the track. After pre-training, we transfer the audio backbone of the model to a set of music audio classification and regression tasks. We demonstrate the usefulness of our approach by comparing the performance of audio representations produced by the same audio backbone with different training strategies and show that our pre-training method consistently achieves comparable or higher scores on all tasks and datasets considered. Our experiments also confirm that MuLaP effectively leverages audio-caption pairs to learn representations that are competitive with audio-only and cross-modal self-supervised methods in the literature.},
	booktitle = {{ICASSP} 2022 - 2022 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, György},
	month = may,
	year = {2022},
	note = {ISSN: 2379-190X},
	keywords = {audio and language, audio representations, multimodal learning, Multiple signal classification, Music, music information retrieval, Natural languages, Signal processing, Supervised learning, Task analysis, Training},
	pages = {456--460},
	file = {IEEE Xplore Full Text PDF:/Users/admin 1/Zotero/storage/G9IXMK5S/Manco et al. - 2022 - Learning Music Audio Representations Via Weak Lang.pdf:application/pdf},
}

@misc{xiaowan_yi_composition-aware_2023,
	title = {Composition-aware music recommendation system for music production},
	author = {{Xiaowan Yi}},
	year = {2023},
}

@phdthesis{chapman_selected_2007,
	type = {phd},
	title = {Selected sounds : a collective investigation into the practice of sample-based music},
	shorttitle = {Selected sounds},
	url = {https://spectrum.library.concordia.ca/id/eprint/975544/},
	abstract = {Selected Sounds involves an ethnographic investigation into the sampling and mixing practices of a group of sound artists from Montreal, Canada. Seven composers (in one case, a composing duo) each contributed a single piece of digitized sound to the project. Each was free to submit any sound they liked, with the one requirement that all samples be selected from sources discovered around Montreal. After collecting the audio files, I placed them together on a CD, copies of which were distributed to every participant. Each composer then put together a track drawing on this sample-pool exclusively for source material. The resulting mixes have been compiled into a nationally distributed, independent audio-CD. All were interviewed regarding the evolution of their knowledge of digital audio production methods, as well as their thoughts on audio sampling.  In Chapter 1 (the Introduction) I present the research project and outline the theoretical framework as well as challenges addressed. In Chapter 2, I discuss various methodological choices made in Selected Sounds as a reflexive ethnography that invokes sample-based musical practices of citation, friendship and creative production as mechanisms for highly-engaged research in the arts, humanities and social sciences. Chapter 3 presents a literature review of recent academic accounts of audio sampling and the impacts of digital technology upon the contemporary reception and use of media. Chapter 4 introduces Appendix A-an interactive database developed from out of the Selected Sounds interview process. Chapter 5 concludes by taking up conversations contained in Appendix A and organizing them around seven different reconfigurations of the concept "sampling" for 2007: sampling-as-technology, sampling-as-community, samplingas-memory, sampling-as-collecting, sampling-as-ethics, and sampling-as-recording.  Appendix A is the Selected Sounds interview database.  Appendix B contains a brief introduction to "The Lesbians on Ecstasy," a musical performing group of which three of the participants in Selected Sounds are members.  Appendix C is a transcript of the final meeting with the Selected Sounds research group},
	language = {en},
	urldate = {2023-07-03},
	school = {Concordia University},
	author = {Chapman, Owen},
	year = {2007},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/324PF6DV/Chapman - 2007 - Selected sounds  a collective investigation into .pdf:application/pdf;Snapshot:/Users/admin 1/Zotero/storage/92ZPN3NJ/975544.html:text/html},
}

@book{saldana_fundamentals_2011,
	title = {Fundamentals of {Qualitative} {Research}},
	isbn = {978-0-19-045379-4},
	abstract = {Fundamentals of Qualitative Research approaches qualitative inquiry as a strategically selected composite of genres, elements, and styles. Saldaña, author of the bestselling book, The Coding Manual for Qualitative Researchers, brings clear writing and explanatory prowess to this new textbook for learning the fundamentals of qualitative research methods. This book presents a concise yet rigorous description of how to design and conduct fieldwork projects and how to examine data in multiple ways for interpretive insight. Saldaña, a master teacher and qualitative data analyst, acquaints readers with the major genres of qualitative research available and the elements of interviewing, participant observation, and other data collection methods to inform emergent research design decisions. An extended chapter on qualitative data analysis is one of the book's unique features. Saldaña devotes necessary coverage to conceptual foundations, coding, analytic memo writing, thematic analysis, assertion development, grounded theory, narrative and poetic inquiry, and ethnodramatic approaches to the data. Eight distinctive styles of qualitative writing are presented. The book concludes with a list of recommended readings in the field, as well as additional resources on organizations and associations dedicated to qualitative research. Fundamentals of Qualitative Research is an ideal introduction for advanced undergraduate and graduate students in education, sociology, psychology, anthropology, human communication, and health care.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Saldana, Johnny},
	month = mar,
	year = {2011},
	note = {Google-Books-ID: MQFdCAAAQBAJ},
	keywords = {Medical / General, Psychology / Clinical Psychology, Psychology / Social Psychology},
}

@misc{noauthor_towards_nodate,
	title = {Towards a {Reflection} in {Creative} {Experience} {Questionnaire} {\textbar} {Proceedings} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	url = {https://dl.acm.org/doi/abs/10.1145/3544548.3581077},
	urldate = {2023-07-03},
	file = {Towards a Reflection in Creative Experience Questionnaire | Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems:/Users/admin 1/Zotero/storage/MJHTM9LI/3544548.html:text/html},
}

@article{barrett_spatio-musical_2002,
	title = {Spatio-musical composition strategies},
	volume = {7},
	issn = {1469-8153, 1355-7718},
	url = {https://www.cambridge.org/core/journals/organised-sound/article/spatiomusical-composition-strategies/03F84FDE41A269DE69DEC873C45FD35D},
	doi = {10.1017/S1355771802003114},
	abstract = {Spatial elements in acousmatic music are inherent to the art form, in composition and in the projection of the music to the listener. But is it possible for spatial elements to be as important carriers of musical structure as the other aspects of sound? For a parameter to serve the requirements of musical development, it is necessary for that parameter to cover a range of perceptually different states. For ‘space’ to be more than a setting within which the main active elements in the structure unfold, it needs to satisfy these requirements. This paper explains a number of important spatial composition strategies available to the acousmatic composer in light of current technology and sound reproduction situations. The analysis takes an aesthetical rather than a technical standpoint.},
	language = {en},
	number = {3},
	urldate = {2023-07-03},
	journal = {Organised Sound},
	author = {Barrett, Natasha},
	month = dec,
	year = {2002},
	note = {Publisher: Cambridge University Press},
	pages = {313--323},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/LCS69QMG/Barrett - 2002 - Spatio-musical composition strategies.pdf:application/pdf},
}


@article{roberts_hierarchical_nodate,
	title = {Hierarchical {Variational} {Autoencoders} for {Music}},
	abstract = {In this work we develop recurrent variational autoencoders (VAEs) trained to reproduce short musical sequences and demonstrate their use as a creative device both via random sampling and data interpolation. Furthermore, by using a novel hierarchical decoder, we show that we are able to model long sequences with musical structure for both individual instruments and a three-piece band (lead, bass, and drums). Finally, we demonstrate the effectiveness of scheduled sampling in signiﬁcantly improving our reconstruction accuracy.},
	language = {en},
	author = {Roberts, Adam and Engel, Jesse and Eck, Douglas},
	file = {Roberts et al. - Hierarchical Variational Autoencoders for Music.pdf:/Users/admin 1/Zotero/storage/SYAXVNEQ/Roberts et al. - Hierarchical Variational Autoencoders for Music.pdf:application/pdf},
}


@article{roberts_hierarchical_2021,
	title = {Hierarchical {Variational} {Autoencoders} for {Music}},
	abstract = {In this work we develop recurrent variational autoencoders (VAEs) trained to reproduce short musical sequences and demonstrate their use as a creative device both via random sampling and data interpolation. Furthermore, by using a novel hierarchical decoder, we show that we are able to model long sequences with musical structure for both individual instruments and a three-piece band (lead, bass, and drums). Finally, we demonstrate the effectiveness of scheduled sampling in signiﬁcantly improving our reconstruction accuracy.},
	language = {en},
	journal = {31st Conference on Neural Information Processing Systems (NIPS 2017)},
	author = {Roberts, Adam and Engel, Jesse and Eck, Douglas},
	year = {2021},
	file = {Roberts et al. - Hierarchical Variational Autoencoders for Music.pdf:/Users/admin 1/Zotero/storage/SYAXVNEQ/Roberts et al. - Hierarchical Variational Autoencoders for Music.pdf:application/pdf},
}


@misc{walshe_ai_2021,
	title = {{AI} {Text} {Scores} - {Royal} {Northern} {College} of {Music}},
	url = {https://www.rncm.ac.uk/research/research-centres-rncm/prism/prism-blog/ai-text-scores/},
	abstract = {Launch of The Text Score Dataset 1.0 by Jennifer Walshe},
	language = {en-GB},
	urldate = {2022-09-19},
	author = {Walshe, Jennifer and Duffy, Sam},
	month = jul,
	year = {2021},
	file = {Snapshot:/Users/admin 1/Zotero/storage/7K3MVT68/ai-text-scores.html:text/html},
}

@article{mcpherson_tahıroğlu_2020, title={Idiomatic Patterns and Aesthetic Influence in Computer Music Languages}, volume={25}, DOI={10.1017/S1355771819000463}, number={1}, journal={Organised Sound}, publisher={Cambridge University Press}, author={McPherson, Andrew and Tahıroğlu, Koray}, year={2020}, pages={53–63}}


@inproceedings{dhanorkar_who_2021,
	address = {New York, NY, USA},
	series = {{DIS} '21},
	title = {Who needs to know what, when?: {Broadening} the {Explainable} {AI} ({XAI}) {Design} {Space} by {Looking} at {Explanations} {Across} the {AI} {Lifecycle}},
	isbn = {978-1-4503-8476-6},
	shorttitle = {Who needs to know what, when?},
	url = {https://doi.org/10.1145/3461778.3462131},
	doi = {10.1145/3461778.3462131},
	abstract = {The interpretability or explainability of AI systems (XAI) has been a topic gaining renewed attention in recent years across AI and HCI communities. Recent work has drawn attention to the emergent explainability requirements of in situ, applied projects, yet further exploratory work is needed to more fully understand this space. This paper investigates applied AI projects and reports on a qualitative interview study of individuals working on AI projects at a large technology and consulting company. Presenting an empirical understanding of the range of stakeholders in industrial AI projects, this paper also draws out the emergent explainability practices that arise as these projects unfold, highlighting the range of explanation audiences (who), as well as how their explainability needs evolve across the AI project lifecycle (when). We discuss the importance of adopting a sociotechnical lens in designing AI systems, noting how the “AI lifecycle” can serve as a design metaphor to further the XAI design field.},
	urldate = {2022-10-20},
	booktitle = {Designing {Interactive} {Systems} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Dhanorkar, Shipi and Wolf, Christine T. and Qian, Kun and Xu, Anbang and Popa, Lucian and Li, Yunyao},
	month = jun,
	year = {2021},
	keywords = {Explainable AI, Interviews, Work Practices},
	pages = {1591--1602},
	file = {Full Text PDF:/Users/admin 1/Zotero/storage/T4NF83TN/Dhanorkar et al. - 2021 - Who needs to know what, when Broadening the Expl.pdf:application/pdf},
}

